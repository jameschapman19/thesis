\chapter{Proximal Gradient Descent: A Flexible, Scalable Approach to Regularised CCA}
\label{Regularised}
\minitoc


\section{Introduction}

\subsection{Problem Statement}
Canonical Correlation Analysis (CCA) is a widely-used technique for finding correlated components between two sets of variables. However, its applicability to large-scale data sets and its robustness to overfitting have been limiting factors. Regularization methods can mitigate these issues, but implementing them in an efficient, scalable manner is not straightforward.

\subsection{Contributions}
The main idea of this chapter is to introduce a novel formulation for regularized CCA using proximal gradient descent algorithms. Our approach allows for the incorporation of various regularization terms, thereby making it highly flexible. We illustrate its scalability and robustness through experiments.

\section{Background}
\subsection{Proximal Gradient Descent}
%TODO - background to Proximal Gradient Descent
\subsection{Total Variation Regularization}
%TODO - background to Total Variation Regularization

\section{Methods}
\subsection{Proximal CCA-EY}
We introduce a formulation called Proximal CCA-EY that extends the Generalized Eigenvalue Problem (GEP) formulation for CCA. By adding a regularization term to the objective function, we can solve this regularized GEP using proximal gradient descent algorithms. This approach permits the use of a variety of regularization functions, which we demonstrate can be efficiently computed. Algorithm details are similar to those shown in Algorithm 1 in the previous section, with an additional step to include the proximal operator corresponding to the chosen regularization term.

\begin{equation}
\label{eq:proximal-CCA-EY}
    \mathcal{U}^{\text{Proximal CCA-EY}}(W) = \mathcal{L}^\text{EY}(W) + \lambda \mathcal{R}(W)
\end{equation}

Here, \(\lambda\) is a hyperparameter that controls the strength of the regularization term, and \(\mathcal{R}(W)\) can be any function for which a proximal operator can be efficiently computed. Specific implementations for L1 and Total Variation regularizations are provided.

\section{Experiments}
To validate our approach, we conduct experiments on synthetic and real-world datasets. We compare our Proximal CCA-EY algorithm against classical CCA and other existing regularized CCA methods in terms of their ability to find meaningful correlations and their resilience to overfitting. Furthermore, we demonstrate the scalability of our approach by applying it to large datasets.

\subsection{Experimental Setup}
The experiments are conducted using Python. We utilize high-performance numerical libraries to ensure efficient execution. The code is made publicly available for reproducibility.

\subsection{Results}
Our method outperforms existing methods in various aspects, including computation time, quality of correlated components, and robustness to overfitting. The flexibility to use different regularization terms makes our approach applicable to a variety of scenarios.

\section{Conclusions}
We have presented a novel method for implementing a flexible, scalable regularized CCA. The use of proximal gradient descent algorithms allows for efficient optimization, making our approach highly suitable for large-scale data analysis. Future work will focus on extending this approach to other variants of CCA and exploring different forms of regularization.



\subsection{Experiment 1: Total Variation Regularisation of Simulated Data}

\subsection{Experiment 2: Total Variation Regularisation of Brain-Behaviour Assocations in the ADNI Data}
\subsection{Experiment Design}
\subsection{Results}

\subsection{Experiment 3: Total Variation Regularisation of Brain-Behaviour Assocations in the ADNI Data}
\subsection{Experiment Design}
\subsection{Results}


\section{Discussion and Conclusion}



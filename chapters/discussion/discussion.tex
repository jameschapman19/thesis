\chapter{Conclusion}\label{ch:discussion}

This chapter provides a summary of the findings of this thesis, discusses their implications, and outlines potential directions for future work. 

\section{Summary of Contributions}

\subsection{Regularisation of CCA Models:
A Flexible Framework based on
Alternating Least Squares}
This chapter presented the FRALS framework for CCA, addressing challenges in analyzing large-scale neuroimaging datasets from projects such as the Human Connectome Project and the Alzheimer's Disease Neuroimaging Initiative. Incorporating structured priors through regularization, particularly the elastic net penalty, FRALS enhanced the interpretability and generalizability of CCA models. This method, documented in the work presented at the OHBM \citep{chapman2023als}, has been effective in uncovering significant brain-behavior associations, showing superior out-of-sample performance compared to traditional methods.

\subsection{Insights From Generating
Simulated Data for CCA}
This chapter contributed to the debate on the interpretation of model weights versus loadings in CCA. By generating high-dimensional simulated data and categorizing methods into explicit and implicit latent variable models, the chapter highlights the robustness of loadings to columnwise transformations in data matrices, a feature not shared with weights. The simulated data strategies formed part of the analysis in \citet{mihalik2022canonical} and influenced the analysis in \citet{ADAMS2024}.

\subsection{Efficient Algorithms for the
CCA Family: Unconstrained
Losses with Unbiased
Gradients}
Focusing on scaling challenges for CCA and PLS in the context of large-scale biomedical datasets like the UK Biobank, this chapter introduces a new gradient descent algorithm tailored for generalized eigenvalue problems. The methods developed, informed by publications \citep{chapman2022generalized, chapman2023efficient, chapman2023cca}, enable the application of multiview CCA and PLS to datasets with extensive dimensions and complex structures.

\subsection{Deep CCA and Self-Supervised
Learning}
This chapter introduces a novel formulation of Deep CCA optimized for the stochastic minibatch setting and proposes SSL-EY, a new competitive SSL method. Grounded in findings from \citep{chapman2023cca} and \citep{chapman2023efficient}, the chapter demonstrates the robustness of these methods against hyperparameter sensitivity and elucidates connections between CCA-based SSL methods and other contemporary SSL approaches.

\subsection{CCA-Zoo: A collection of Regularized, Deep Learning-based, Kernel, and Probabilistic methods in a scikit-learn style framework}
Presenting \texttt{CCA-Zoo}, a Python library that consolidates and enhances the accessibility of multiview learning methods, this chapter details the development and capabilities of the library, which implements a variety of CCA, PLS, and related techniques. As detailed in \citep{chapman2021cca}, \texttt{CCA-Zoo} addresses gaps in existing software offerings and facilitates broader adoption and innovation within the research community.

In summary, we have demonstrated novel ways to introduce structured priors into CCA models, developed efficient algorithms for large-scale CCA, extended CCA to deep learning, and provide a unified interface for various CCA methods. Finally, we have made software implementations of these methods available to the research community through the \texttt{CCA-Zoo} package which have already been well-received by the community.

\section{Limitations and Challenges}

It is important to note that while this thesis presents significant methodological advancements and software contributions, the new applied results are somewhat limited. This limitation was largely due to unforeseen circumstances, including significant bureaucratic challenges in accessing the UK Biobank dataset and the global impact of the COVID-19 pandemic. These factors restricted our ability to conduct extensive applied research as initially planned. However, this limitation has been partially offset by the broad adoption and application of our software tools by other researchers in various fields, extending the impact of our work beyond our own direct applications.

\section{Future Work}

\subsection{Applications}

\subsubsection{Large-Scale Neuroimaging Datasets}

While the applications presented in this thesis, particularly the UK Biobank analysis in Chapter \ref{ch:deep_learning}, have demonstrated the potential of our methods, there is still vast untapped potential in applying these techniques to even larger and more diverse datasets. The ABCD dataset, for instance, offers a rich source of multimodal data that could benefit from the regularized and scalable CCA methods developed in this thesis. Preliminary results on this dataset have shown promise, and we believe that further exploration will yield valuable insights into brain development and its associated factors.

\subsubsection{Wearable Devices}

The rise of wearable devices and the proliferation of biometric data present new opportunities for applying multiview learning techniques to personal health monitoring. By integrating data streams from devices such as smartwatches, continuous glucose monitors, and sleep trackers, we can gain insights into an individual's physical and mental well-being that were previously inaccessible. I strongly believe that the development of interpretable and scalable methods for analyzing these diverse data sources will be crucial for unlocking the full potential of wearable technology in personalized healthcare.

\subsection{Methods}

\subsubsection{Proximal Gradient Descent for Regularized CCA}

Preliminary experiments suggest that the proximal gradient descent approach discussed in chapter \ref{ch:gradient_descent} is much faster than existing methods, making it a promising direction for future research. We anticipate that this methodology will significantly enhance the scalability and applicability of CCA, PCA, and PLS in the era of big data and complex regularization schemes.

\section{Closing Remarks}

My PhD journey has been a fascinating exploration of the world of multiview learning, with Canonical Correlation Analysis at its core. What began as an endeavor to apply deep learning to uncover brain-behavior associations evolved into a multifaceted exploration, leading to the development of scalable algorithms for linear CCA and the investigation of connections between CCA and self-supervised learning.

Confronting the scalability bottleneck head-on led me to develop these efficient algorithms, pushing the boundaries of CCA and PLS methods. This exploration also unveiled intriguing connections between CCA and self-supervised learning, bringing the journey full circle.

Witnessing the rapid growth and evolution of multiview learning during my PhD has been incredibly stimulating. From the emergence of powerful multimodal language models to the increasing adoption of self-supervised learning techniques, being part of this dynamic field has been a privilege. This journey has transformed me as a researcher, honing my skills in algorithm development, software engineering, and bridging theoretical concepts with practical applications.

One of the most rewarding aspects of this work has been its impact, particularly through the CCA-Zoo package. Seeing researchers across various fields use these tools to tackle diverse problems has been immensely gratifying, underscoring the importance of accessible method implementations. Moreover, the potential applications of these methods in areas like large-scale neuroimaging studies and wearable device data analysis hint at broader impacts on our understanding of human health and behavior.

Looking ahead, I believe the future of multiview learning holds immense promise. The integration of deep learning with CCA and the application of these methods to ever-larger and more diverse datasets open exciting avenues for research. Particularly promising are the applications to large-scale neuroimaging datasets and the analysis of multimodal data from wearable devices for personalized healthcare.

Thank you for reading.
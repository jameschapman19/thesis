\chapter{Flexible Regularization for CCA}
\label{chap:als}
\minitoc

\section{Introduction}\label{sec:introduction}

This chapter is an extension of work that I have previously presented in poster form at the OHBM conference.
I have benefitted from constructive theoretical discussions with John Shawe-Taylor and Janaina Mourao-Miranda, as well as insights into result interpretation from Janaina and Rick Adams.

The core focus of this part of my PhD research was understanding and improving the regularization in Canonical Correlation Analysis (CCA) models.
Traditional CCA models often exhibit shortcomings when grappling with high-dimensional data, a challenge particularly acute in the context of brain-behavior studies.
Firstly, they are prone to overfitting, leading to spurious correlations and poor generalization.
Secondly, even when the correlations are genuine, the models are often difficult to interpret, making it hard to draw meaningful conclusions. This is especially problematic in the context of brain-behavior studies, where the goal is to understand the relationship between brain activity and behavior and we are less interested in predicting the latent variables themselves.

\subsection{Problem Statement}\label{subsec:problem-statement}

We aim to tackle two key challenges with traditional CCA methods: overfitting and lack of interpretability. By focusing on the role of model priors, we use simulated and real data to demonstrate how optimal performance is achieved when the model priors align with the data-generating priors. We also explore the trade-offs between regularizing weights for better out-of-sample correlations and maintaining interpretability.

\subsection{Contributions}\label{subsec:contributions}

Our contributions are twofold. First, we advocate for a more principled approach to data generation in CCA studies, highlighting the importance of understanding the underlying assumptions. This leads us to recommend focusing on loadings for interpretability when the goal is to understand associations rather than predict latent variables. 

Second, we introduce Flexible Regularized Alternating Least Squares (FRALS), a flexible and robust framework that can incorporate any regularized least squares solver, thus providing an effective defense against overfitting. Our experiments reveal both the promising aspects of FRALS in terms of out-of-sample correlation and its computational limitations, offering a roadmap for future research in this area.


\section{Background}\label{sec:background}

\subsection{How is CCA and PLS Data Generated and Why Should We Care?}\label{sec:how-is-cca-and-pls-data-generated-and
-why-should-we-care?}

Understanding the data generation process in Canonical Correlation Analysis (CCA) and Partial Least Squares (PLS) is pivotal for many reasons. It influences the choice of appropriate models, evaluation metrics, and sheds light on the underlying structure and dependencies between views. Probabilistic formulations provide a principled framework to understand this process, helping us gauge the assumptions we make and the limitations these impose.


\subsection{Data Generation}\label{subsec:data-generation-background}

There are a number of ways used in the Sparse CCA literature to generate synthetic data.

\subsubsection{Simple Latent Variable Model}

Witten\cite{witten2009extensions} used a generative model of the form defined by:

\begin{align}\label{eq:wittengen}
    & \bold{X_i}=\bold{z_kw_{ki}}+\epsilon
\end{align}

which we can represent as the graphical model in figure~\ref{fig:wittengraphical}.

\begin{figure}
\centering
  \tikz{
%nodes
 \node[latent, align=center] (Z) {$Z$}; %
 \node[obs, below left=of Z] (X\sps{1}) {$X\sps{1}$};%
 \node[obs, below right=of Z] (X\sps{2}) {$X\sps{2}$};%
 \node[latent,above left=of X\sps{1},fill] (W\sps{1}) {$W\sps{1}$}; %
 \node[latent,above right=of X\sps{2},fill] (W\sps{2}) {$W\sps{2}$}; %
 \node[latent,left=of X\sps{1}] (sigma_1) {$\sigma_1$}; %
 \node[latent,right=of X\sps{2}] (sigma_2) {$\sigma_2$}; %
% edges
 \edge {W\sps{1},Z,sigma_1} {X\sps{1}}
 \edge {W\sps{2},Z,sigma_2} {X\sps{2}}}
 \caption[Sparse Data Generation Graphical Model]{\textit{\textbf{Sparse Data Generation Graphical Model: }}For the Witten data generation, we set the active variables with the matrices $W_i$ and therefore rescale the latent variable Z in those variables with zeros in the other variables. Z has standard normal distribution. We add gaussian noise parameterized by $\sigma$.}
 \label{fig:wittengraphical}
\end{figure}

While this simple method directly controls the cross-covariance it doesn't control the within dataset covariance $\bold{\Sigma_{ii}}$.

Indeed, by marginalising out the latent variables $\bold{Z}$ we can write down the joint distribution of the two views as:

\begin{align}
    \begin{bmatrix} \bold{X\sps{1}} \\ \bold{X\sps{2}} \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} \bold{0} \\ \bold{0} \end{bmatrix}, \begin{bmatrix} \bold{W\sps{1}W\spstop{1}} + \sigma_1^2\bold{I} & \bold{W\sps{1}W\spstop{2}} \\ \bold{W\sps{2}W\spstop{1}} & \bold{W\sps{2}W\spstop{2}} + \sigma_2^2\bold{I} \end{bmatrix} \right)
\end{align}

\subsubsection{Joint Covariance Matrix}
Later papers in the sparse CCA literature such as~\cite{mai2019iterative,chen2013sparse} directly contstruct the joint covariance and sample from this multivariate normal distribution to give two simulated views with specified active variables, correlations and within-view covariance.
This has the advantage of allowing us to control the within-view covariance and therefore test the methods under specific conditions.
The process was first described by Chen~\cite{chen2013sparse} and further explained by~\cite{suo2017sparse}.

We construct the joint distribution as follows:

\begin{align}\label{eq:covariance}
    \begin{bmatrix} \bold{X\sps{1}} \\ \bold{X\sps{2}} \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} \bold{0} \\ \bold{0} \end{bmatrix}, \begin{bmatrix} \bold{\Sigma_{11}} & \bold{\Sigma_{12}} \\ \bold{\Sigma_{21}} & \bold{\Sigma_{22}} \end{bmatrix} \right)
\end{align}

Where $\bold{\Sigma_{11}}$ and $\bold{\Sigma_{22}}$ are the within-view covariance matrices and $\bold{\Sigma_{12}}$ and $\bold{\Sigma_{21}}$ are the between-view covariance matrices.

We can control the true signal by setting the active variables and correlations in the between-view covariance
matrices $\bold{\Sigma_{12}}$ and $\bold{\Sigma_{21}}$. Specifically

\begin{align}
    \bold{\Sigma_{12}}=\sum_{k=1}^{K}\rho_k\Sigma_{11}u\sps{1}_{k}u\sps{2\top}_k\Sigma_{22}
\end{align}

Where $\rho_k$ is the $k$th canonical correlation and $u\sps{i}_k$ is the $k$th column of $\bold{U_i}$.

\subsubsection{Probabilistic CCA}

In Probabilistic CCA~\cite{bach2005probabilistic}, the generative model is articulated as a two-step process for each view \(i\):

\begin{align}
    \mathbf{z}& \sim \mathcal{N}(\mathbf{0}, \mathbf{I})                                            \\
    \mathbf{x}_i & \sim \mathcal{N}(\mathbf{W}_i \mathbf{z} + \boldsymbol{\mu}_i, \boldsymbol{\Psi}_i)
\end{align}

Here, \(\mathbf{z}\) represents the latent variables shared between the different views. \(\mathbf{x}_i\) is generated conditionally based on \(\mathbf{z}\), and can be thought of as the observed data with additional noise \(\boldsymbol{\Psi}_i\).

% TikZ Diagram for Probabilistic CCA
\begin{tikzpicture}
    % Define nodes
    \node[latent] (z) {z};
    \node[obs, below=of z] (x1) {\(X\sps{1}\)};
    \node[obs, right=of x1] (x2) {\(X\sps{2}\)};
    % Connect nodes
    \edge {z} {x1,x2} ;
    % Plates
    \plate {} {(x1)(x2)} {Views}
\end{tikzpicture}

Once again, by marginalizing out the latent variables \(\mathbf{z}\), we can write down the joint distribution of the two views as:

\begin{align}
    \begin{bmatrix} \bold{X\sps{1}} \\ \bold{X\sps{2}} \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} \bold{\mu\sps{1}} \\ \bold{\mu\sps{2}} \end{bmatrix}, \begin{bmatrix} \bold{W\sps{1}W\spstop{1}} + \bold{\Psi_1} & \bold{W\sps{1}W\spstop{2}} \\ \bold{W\sps{2}W\spstop{1}} & \bold{W\sps{2}W\spstop{2}} + \bold{\Psi_2} \end{bmatrix} \right)
\end{align}

\subsubsection{Probabilistic rCCA}

Probabilistic rCCA~\cite{de2003regularization} extends Probabilistic CCA by incorporating regularization directly into the covariance structure:

\begin{align}
    \mathbf{z}& \sim \mathcal{N}(\mathbf{0}, \mathbf{I})                                            \\
    \mathbf{x}_i & \sim \mathcal{N}(\mathbf{W}_i \mathbf{z} + \boldsymbol{\mu}_i, \boldsymbol{\Psi}_i + \Sigma_i^2)
\end{align}

Here, \(\Sigma_i^2\) is a diagonal matrix that models measurement noise. This addition allows the model to be more
robust to overfitting and other data challenges. This gives us the following joint distribution:

\begin{align}
    \begin{bmatrix} \bold{X\sps{1}} \\ \bold{X\sps{2}} \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} \bold{\mu\sps{1}} \\ \bold{\mu\sps{2}} \end{bmatrix}, \begin{bmatrix} \bold{W\sps{1}W\spstop{1}} + \bold{\Psi_1} + \bold{\Sigma_1^2} & \bold{W\sps{1}W\spstop{2}} \\ \bold{W\sps{2}W\spstop{1}} & \bold{W\sps{2}W\spstop{2}} + \bold{\Psi_2} + \bold{\Sigma_2^2} \end{bmatrix} \right)
\end{align}

\subsubsection{Probabilistic PLS}

Probabilistic formulations of PLS are still an evolving field \cite{el2018probabilistic,zheng2016probabilistic}.
Despite various models for PLS regression, a canonical probabilistic model for Canonical PLS remains elusive.

\subsection{Summary of Data Generation Methods}

\begin{table}[h]
    \centering
    \caption{Summary of Data Generation Methods}
    \begin{tabular}{l|c|c|c|}
        \textbf{Method} & \textbf{Within-View Covariance} & \textbf{Between-View Covariance} & \textbf{Active
        Variables} \\
        \hline
        Witten           & No                              & No                               & No                        \\
        Covariance       & Yes                             & Yes                              & Yes                       \\
        Probabilistic CCA & Yes                             & Yes                              & Yes                       \\
        Probabilistic rCCA & Yes                             & Yes                              & Yes                       \\
        \hline
    \end{tabular}
    \label{table:data-generation-methods-properties}
\end{table}

% table with the joint covariance matrix for each method

\begin{table}[h]
    \centering
    \caption{Summary of Data Generation Methods}
    \begin{tabular}{l|c|}
        Method & Joint Covariance  \\
        \hline
        Witten & \( \begin{bmatrix} X\sps{1} \\ X\sps{2} \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} W\sps{1}W\spstop{1} + \sigma_1^2 I & W\sps{1}W\spstop{2} \\ W\sps{2}W\spstop{1} & W\sps{2}W\spstop{2} + \sigma_2^2 I \end{bmatrix} \right) \) \\
        Covariance & \( \begin{bmatrix} X\sps{1} \\ X\sps{2} \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix} \right) \) \\
        Probabilistic CCA & \( \begin{bmatrix} X\sps{1} \\ X\sps{2} \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} \mu^{1} \\ \mu^{2} \end{bmatrix}, \begin{bmatrix} W\sps{1}W\spstop{1} + \Psi_1 & W\sps{1}W\spstop{2} \\ W\sps{2}W\spstop{1} & W\sps{2}W\spstop{2} + \Psi_2 \end{bmatrix} \right) \) \\
        Probabilistic rCCA & \( \begin{bmatrix} X\sps{1} \\ X\sps{2} \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} \mu^{1} \\ \mu^{2} \end{bmatrix}, \begin{bmatrix} W\sps{1}W\spstop{1} + \Psi_1 + \Sigma_1^2 & W\sps{1}W\spstop{2} \\ W\sps{2}W\spstop{1} & W\sps{2}W\spstop{2} + \Psi_2 + \Sigma_2^2 \end{bmatrix} \right) \) \\
        \hline
    \end{tabular}
    \label{table:data-generation-methods}
\end{table}


\section{Contributions}\label{sec:contributions}

\subsection{Clarifying the use of weights and loadings in CCA}

Our first contribution is to clarify the use and intepretation of weights and loadings in CCA.

\subsubsection{Forward and Backward Models}

At this stage we can consider the difference between forward and backward models in the context of CCA and PLS.
In forward models we generate data from a known model and then try to recover the parameters of that model.
In backward models we try to recover the parameters of a model from the data.

In the CCA and PLS models described in chapter \ref{chap:background} we have backward models which aim to infer the
latent variables $\bold{Z}$ from the data $\bold{X_i}$.
In the probabilistic models described in section \ref{subsec:probabilistic-cca-rcca-and-pls} we have forward models which
generate data from the latent variables $\bold{Z}$.

\subsubsection{Why is this important?}

If we are solely interested in the latent variables $\boldsymbol{Z}$, then the backward models described in chapter \ref{chap:background} suffice for inferring them from the data.
However, if we aim for a comprehensive understanding and interpretability of the model, it becomes vital to understand both forward and backward formulations.

Consider the probabilistic CCA model described in section \ref{subsec:probabilistic-cca-rcca-and-pls}, but with concrete names for the latent variables and views. Let's take figure \ref{fig:mentalhealthselfsupervised} as an example where the latent variable represents the severity of a mental health condition, and the views (neuroimaging and behavioral data) are conditioned on this latent variable.


\begin{figure}
    \centering
    \tikz{
        % nodes
        \node[latent, align=center, minimum size=2cm] (Z) {Severity};
        %
        \node[obs, below left=of Z, minimum size=2cm] (X\sps{1}) {Neuroimage};%
        \node[obs, below right=of Z, minimum size=2cm] (X\sps{2}) {Behaviour};%
        % edges
        \edge{Z} {X\sps{1}}
        \edge{Z} {X\sps{2}}}
    \caption[Latent Variable Model of Mental Health]{\textit{\textbf{Latent Variable Model of Mental Health:}} From this perspective the neuroimaging modality and behavioural data are both considered to have been generated with distributions conditioned on the severity of a mental health condition}\label{fig:mentalhealthselfsupervised}
\end{figure}

In both the forward and backward models of CCA, the latent variables are of interest and are mathematically identifiable (up to a rotation), meaning that although they may be represented differently, they capture the same underlying structure.
However, their conceptual implications differ between the two models.
In the forward model, the weights indicate how the latent variables influence the generation of the views.
Conversely, in the backward model, the weights help us best predict or reconstruct the latent variables from the data.

One of our primary contributions in this work is shedding light on the intricate relationship between the forward and backward models of CCA, particularly with regard to the weights involved.
Both the forward and backward models provide analytical forms for the joint covariance matrix of the views:

\begin{align}
    \Sigma &= \begin{bmatrix}
        \bold{W\sps{1}W\spstop{1}} + \bold{\Psi_1} & \bold{W\sps{1}W\spstop{2}} \\
        \bold{W\sps{2}W\spstop{1}} & \bold{W\sps{2}W\spstop{2}} + \bold{\Psi_2}
    \end{bmatrix} = \begin{bmatrix}
        \bold{\Sigma_1} & \bold{\Sigma_1}\sum_k \rho_k u_{1_k}^\top u_{2_k} \bold{\Sigma_2}  \\
        \bold{\Sigma_2}\sum_k \rho_k u_{2_k}^\top u_{1_k} \bold{\Sigma_1} & \bold{\Sigma_1}
    \end{bmatrix}
\end{align}

Here, $\rho_k$ represents the $k$th canonical correlation, and $u_{i_k}$ denotes the $k$th column of $\bold{U_i}$.

Our crucial insight lies in the relationship between the weights of the forward and backward models, which can be expressed as:

\begin{align}
    \bold{W\sps{1}W\spstop{2}} &= \bold{\Sigma_1}\sum_k \rho_k u_{1_k}^\top u_{2_k} \bold{\Sigma_2}
\end{align}

For a single latent variable, this leads to the following relationships:

\begin{align}
    \bold{W\sps{1}} &= p_1\bold{\Sigma_1} u_{1_1}^\top \\
    \bold{W\sps{2}} &= p_2\bold{\Sigma_2} u_{2_1}^\top
\end{align}

Here, $p_i$ represents scalar constants, and it's important to note that the product of these constants, $\prod_i p_i$ equals $\rho$.
Interestingly, this relationship highlights that the weights in the backward model are scaled by the covariance matrices $\bold{\Sigma_i}$, making them closely akin to loadings.

It's worth noting that this revelation underscores the significance of covariance matrices in determining the relationships between forward and backward weights.
Specifically, when the covariance matrices are identity matrices, the forward and backward weights become identical (up to scaling by $p$).
Furthermore, it's important to recognize that sparse weights do not necessarily imply sparse loadings and vice versa, unless the covariance matrices themselves are identity matrices.


\subsection{Flexible Regularized Alternating Least Squares (FRALS)}\label{subsec:flexible-regularized-alternating-least
-squares-(frals)}
We adopt an alternating minimization strategy to solve the CCA problem.
The objective is to minimize the sum of squared Frobenius norms between the latent variables and their projections in each view.
Regularization terms are added to the objective function to avoid overfitting.
Our formulation allows the incorporation of any regularized least squares solver, making it extremely flexible.

\subsubsection{Mathematical Formulation}
We can solve CCA by alternating minimization over each view, based on the alternating least squares form.
This form finds a variable \( T \) that is close to the latent variables \( X\sps{1} U\sps{1} \), where \( X\sps{1} \), \( U\sps{1} \) are the matrix and weights for each view \( i \).
The closer \( T \) is to \( X\sps{1} U\sps{1} \), the higher the correlation between them.
The constraint \( T^\top T = I \) ensures that the latent space is orthogonal to find different effects.

\begin{gather*}
    \underset{U, T}{\mathrm{argmin}}\left\{\sum_i \|X\sps{i} U\sps{i} - T\|_F^2 \right\}\\
    \text{subject to: } T^\top T = I\\
\end{gather*}

To regularize the projection matrices, we add penalty terms to the objective function, such as \( P(U\sps{1}) = \lambda_i \|U\sps{1}\|_F \) for ridge regression or \( P(U\sps{1}) = \lambda_i \|U\sps{1}\|_1 \) for lasso.
This can help us avoid overfitting and improve the interpretability of the results.
This means that \textbf{any regularized least squares solver} can be used to solve each subproblem, such as ridge regression, lasso, elastic net, etc., making our framework substantially more flexible than prior work.

\begin{gather*}
    \underset{U, T}{\mathrm{argmin}}\left\{\sum_i \|X\sps{i} U\sps{i} - T\|_F^2 + \textcolor{red}{\lambda_i P(U\sps{i})}\right\}\\
    \text{subject to: } T^\top T = I\\
\end{gather*}


\section{Experiment 1: Weights vs Loadings}\label{sec:exp1}

In this experiment, we generate data according to the probabilistic CCA model where we control the loadings as well
as the joint covariance method where we control the weights and canonical correlations.

In doing so we will illustrate that

\subsection{Set-Up}\label{subsec:set-up}




\section{Experiment 2: Regularized CCA with Simulated Data}

\subsection{Set-Up}\label{subsec:set-up}
In this experiment, we evaluate several Sparse Canonical Correlation Analysis (CCA) variants using synthetic data generated with the FRALS framework. The variants studied include IPLS+ and ElasticNet, both implemented within the FRALS framework for added flexibility.

\subsubsection{Intuition for Model Comparison}
IPLS+ assumes a prior about the data generation process, specifically that the true weights are positive, aligning with the synthetic data. ElasticNet, while flexible, may not align with this specific prior.

\subsubsection{Simulated Data Generation}\label{subsubsec:simulated-data-generation}
The synthetic data is generated using the \texttt{LinearSimulatedData} class from the CCA-Zoo Python package, with parameters set to emulate a high-dimensional feature space.

\begin{table}[h]
\centering
\caption{Experiment Parameters}
\begin{tabular}{l|l}
\textbf{Parameter} & \textbf{Value} \\
\hline
Number of samples (\textit{n}) & 500 \\
Number of features in View 1 (\textit{p}) & 200 \\
Number of features in View 2 (\textit{q}) & 200 \\
Latent dimensions & 1 \\
Sparsity in View 1 & 0.1 \\
Sparsity in View 2 & 0.1 \\
Target correlation between views & 0.9 \\
\end{tabular}
\label{table:experiment-parameters}
\end{table}

\subsubsection{Methodology}
We employ several CCA variants for this experiment, including Canonical Correlation Analysis (CCA), Partial Least Squares (PLS), and more. Grid search is employed for hyperparameter tuning for Elastic CCA and SCCA\_PMD models.

\begin{table}[h]
\centering
\caption{Employed CCA Variants}
\begin{tabular}{l}
\textbf{CCA Variant} \\
\hline
Canonical Correlation Analysis (CCA) \\
Partial Least Squares (PLS) \\
Sparse CCA via Penalized Majorization-Minimization (SCCA\_PMD) \\
Sparse CCA via Iterative Penalized Least Squares (SCCA\_IPLS) \\
Sparse CCA via Span Regularization (SCCA\_Span) \\
Elastic CCA \\
\end{tabular}
\label{table:cca-variants}
\end{table}

\subsection{Results}
\subsubsection{Performance Metrics}
The models are evaluated based on their correlation score on a validation set comprising 20\% of the original data.

\subsubsection{True Weights for Context}

Figure~\ref{fig:True_weights} provides a ground truth reference for the weights in the true underlying signal, setting the context for our subsequent analysis.

\begin{figure}[h]
    \centering
    \includesvg[width=0.6\textwidth]{figures/als/simulated/True_weights.svg}
    \caption{Ground truth weights for the true underlying signal.}
    \label{fig:True_weights}
\end{figure}

\subsubsection{Interpretation of Weight Plots}

Figures~\ref{fig:CCA_weights} to~\ref{fig:IPLS+_weights} visualize the weights attributed to each feature by different models.
Different colors indicate the indices of weights involved in the true signal (as shown in Figure~\ref{fig:True_weights}) and those not involved.

\subsubsection{Performance Insights}

\paragraph{Impact of Sparsity:}
Figure~\ref{fig:PLS_weights} and Figure~\ref{fig:CCA_weights} display the weights for PLS and CCA, respectively.
Both models lack sparsity priors and hence select all features with non-zero weights.
This dilutes the true signal and results in lower validation correlation.

\begin{figure}[h]
    \centering
    \includesvg[width=0.6\textwidth]{figures/als/simulated/PLS_weights.svg}
    \caption{PLS selects all features with non-zero weights, leading to diluted true signals.}
    \label{fig:PLS_weights}
\end{figure}

\begin{figure}[h]
    \centering
    \includesvg[width=0.6\textwidth]{figures/als/simulated/CCA_weights.svg}
    \caption{ Similar to PLS, CCA also lacks a sparsity prior, leading to poor feature selection.}
    \label{fig:CCA_weights}
\end{figure}

\paragraph{Objective Mismatch:}
In contrast, PMD incorporates a sparsity prior. However, as seen in Figure~\ref{fig:PMD_weights}, its objective is to maximize covariance, not correlation. This leads to its middling performance.

\begin{figure}[h]
    \centering
    \includesvg[width=0.6\textwidth]{figures/als/simulated/PMD_weights.svg}
    \caption{PMD shows sparsity but focuses on maximizing covariance rather than correlation.}
    \label{fig:PMD_weights}
\end{figure}

\paragraph{Efficacy of IPLS:}
IPLS performs well due to its alternating lasso procedure and sparsity priors on each view. Figure~\ref{fig:IPLS_weights} confirms its ability to better capture the true underlying signal.

\begin{figure}[h]
    \centering
    \includesvg[width=0.6\textwidth]{figures/als/simulated/SCCA_IPLS_weights.svg}
    \caption{IPLS captures the true underlying signal effectively, thanks to its sparsity priors.}
    \label{fig:IPLS_weights}
\end{figure}

\paragraph{Advantage of Elastic Net:}
Elastic Net regularization combines both $l_1$ and $l_2$ penalties. As Figure~\ref{fig:ElasticNet_weights} shows, this strikes a good balance between feature selection and signal capture.

\begin{figure}[h]
    \centering
    \includesvg[width=0.6\textwidth]{figures/als/simulated/ElasticCCA_weights.svg}
    \caption{Elastic Net shows balanced feature selection and effective signal capture.}
    \label{fig:ElasticNet_weights}
\end{figure}

\paragraph{Superiority of IPLS+:}
As illustrated in Figure~\ref{fig:IPLS+_weights}, IPLS+ with a positive constraint shows the best performance among the models tested, emphasizing the benefit of domain-specific priors.

\begin{figure}[h]
    \centering
    \includesvg[width=0.6\textwidth]{figures/als/simulated/SCCA_IPLS_positive_weights.svg}
    \caption{IPLS+ outperforms other models, benefiting from a positive constraint.}
    \label{fig:IPLS+_weights}
\end{figure}

\subsection{Discussion}
Our results emphasize the importance of aligning the model's objective with the characteristics of the underlying signal for effective feature selection and signal capture. Sparsity and domain-specific priors play a significant role, with IPLS+ showing the best performance among the models tested due to its positive constraint.


\section{Experiment 3: Regularized CCA with the Human Connectome Project (HCP) Data}

The Human Connectome Project (HCP) provides high-dimensional, multimodal data that is challenging to model, making it an ideal testing ground for evaluating the flexibility and efficacy of our FRALS method.

\subsection{Set-Up}
In our study, we used resting-state fMRI along with non-imaging subject measures from a total of 1003 healthy subjects, sourced from the 1200-subject data release of the Human Connectome Project (HCP). The dataset was divided into an 80\% training set and a 20\% testing set.
To fine-tune the regularization of hyperparameters, cross-validation was employed within the training data.
Our approach, dubbed Flexible Regularized Alternating Least Squares (FRALS) with elastic net regularization on both views, was compared against existing methods, namely separate PCA on each modality, Penalized Matrix Decomposition (PMD), and Ridge Regularized CCA (RCCA). The performance was evaluated based on out-of-sample correlations.

\subsubsection{Performance Metrics}
Out-of-sample canonical correlations were chosen as the primary metric as they provide a robust evaluation of how well the model generalizes to new, unseen data.

\subsection{Results}
Our model, FRALS, demonstrated superior performance by achieving the highest out-of-sample canonical correlations among all models tested (see Figure~\ref{fig:performance}).

\begin{figure}[h]
\centering
\includesvg[width=0.5\linewidth]{figures/als/hcp/barcorr}
\caption{Out-of-sample canonical correlations for each model.}
\label{fig:performance}
\end{figure}

\subsubsection{Model Similarities}
We computed the correlation matrix of the scores for each modality to evaluate the similarity of the latent variables learned by each model.
FRALS exhibited low or negative correlations with other models, highlighting its ability to capture novel and distinct aspects of brain-behaviour associations (see Figure~\ref{fig:similarities}).

\begin{figure}[h]
\centering
\includegraphics[width=0.49\linewidth]{figures/als/hcp/behaviour_model_similarities}
\includegraphics[width=0.49\linewidth]{figures/als/hcp/brain_model_similarities}
\caption{Left: Correlation matrix of the scores for each modality. Right: Correlation matrix of the brain loadings for each model.}
\label{fig:similarities}
\end{figure}

\subsubsection{Behaviour and Brain Loadings}
In terms of behavioral loadings, except for PCA, all models identified a latent variable that correlated positively with cognitive tests and negatively with cigarette, tobacco, or alcohol use.
Both RCCA and FRALS demonstrated stronger correlations with the Line Orientation test, which measures visuospatial abilities.

\begin{figure}[h]
\centering
\includesvg[width=\linewidth]{figures/als/hcp/all_top_and_bottom_loadings.svg}
\caption*{Top 5 positive and negative non-imaging loadings for each model}
\label{fig:behaviour}
\end{figure}

Regarding the brain loadings, our analysis shows that each model assigns different weights to various brain regions
based on their connectivity.
RCCA and FRALS assigned more weight to the parietal lobe, known for its role in visuospatial processing, than did PCA and PMD. This suggests that the parietal lobe is more relevant for the brain-behaviour correlations captured by our model.
Conversely, PMD appears to rely on principal components in the brain, potentially missing the true associations between the views.
FRALS functions as a sparse version of RCCA in this context.

\begin{figure}[h]
\centering
\includegraphics[width=0.49\linewidth]{figures/als/hcp/pca_brain_loadings}
\includegraphics[width=0.49\linewidth]{figures/als/hcp/pmd_brain_loadings}
\includegraphics[width=0.49\linewidth]{figures/als/hcp/rcca_brain_loadings}
\includegraphics[width=0.49\linewidth]{figures/als/hcp/flexals_brain_loadings}
\caption*{Map of CCA connection strength increases/decreases, with each nodeâ€™s parcel map weighted by CCA edge-strength increases, summed across edges involving that node.}
\label{fig:brain}
\end{figure}

\subsection{Discussion and Limitations}

While FRALS offers promising performance in terms of out-of-sample correlation, it does come with significant drawbacks, the most noteworthy being its computational inefficiency. Below, we outline the primary factors contributing to the slow speed of FRALS and provide some insights into the computational bottlenecks.

\subsubsection{Computational Time}\label{subsec:computational-time}
While computationally intensive, the flexibility of FRALS allows it to adapt better to the complexity inherent in real-world data sets, such as the HCP. This adaptability could be crucial when high predictive accuracy or interpretability is required.

\subsubsection{Changing Regression Targets}\label{subsec:changing-regression-targets}
Adding to the computational burden is the fact that the regression targets, i.e., the projections of the other view, are not static but change dynamically throughout the algorithm's run.
Each update to the least squares solution consequently alters the global objective, leading to a constantly shifting landscape that the algorithm needs to navigate.


\section{Conclusions}
The main idea of this chapter is to introduce FRALS, a novel approach in the realm of multiview learning that optimizes
both flexibility and performance.
Our experiments indicate that FRALS not only outperforms established methods like PCA, PMD, and RCCA in terms of out-of-sample canonical correlations but also captures novel and distinct aspects of brain-behavior associations.
This uniqueness is evident from the low or negative correlations FRALS holds with other models.

Our findings also underline the importance of the parietal lobe in understanding brain-behavior associations.
FRALS emphasizes this region more compared to traditional methods like PCA and PMD. Future work should focus on understanding the specific functions and contributions of different brain regions captured by FRALS and how they relate to various behaviors.

Given its promising initial results, the next step for FRALS would be its application to larger datasets and its adaptation for different kinds of biological and non-biological data to further evaluate its robustness and applicability.







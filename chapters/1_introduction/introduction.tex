\chapter{Introduction}\label{Introduction}
\minitoc
\section{Setting the Stage: Biomedical Data and The Need for Effective Analytical Techniques}

The advent of modern technologies in biomedical research has led to an unprecedented surge in the quantity and complexity of data generated. From genomic sequences and proteomic profiles to neuroimaging and electronic health records, these multilayered datasets are a treasure trove of potential insights into disease processes and patient health. The ultimate goal of deciphering these intricate datasets is to enhance patient stratification processes, enabling a move towards personalised, precision healthcare. However, the analytical challenges posed by the scale, complexity, and heterogeneity of these datasets, coupled with the scarcity of labelled data, are significant.

In light of these challenges, the main idea of this thesis is to provide scalable, flexible, and interpretable methods in multiview self-supervised learning for biomedical data. Specifically, we propose new methodologies based on unsupervised and self-supervised learning to make sense of these large, unlabelled, and complex datasets. It presents a specific focus on Canonical Correlation Analysis (CCA), a widely used technique in the area of multi-view self-supervised learning (SSL), which, while powerful, comes with its own set of limitations when dealing with large-scale biomedical data.

\section{Problem Statement: The Need for Scalable, Flexible, and Interpretable Multiview SSL Methods}

Despite the advantages offered by multiview SSL methods in handling complex and high-dimensional biomedical data, the existing methodologies face substantial limitations, particularly concerning scalability, flexibility, and interpretability. The computational costs of traditional algorithms for solving CCA and other generalized eigenvalue problems are prohibitive when dealing with large-scale datasets. This limitation directly impacts their scalability and applicability to modern biomedical datasets. Moreover, the inherent inflexibility of traditional methods fails to capture the nonlinear relationships often present in biomedical data.

\section{Thesis Structure and Contributions}

The main contributions of this thesis are threefold: 1) A scalable reformulation of Canonical Correlation Analysis (CCA); 2) Incorporation of regularization techniques for improved interpretability; and 3) Preliminary groundwork for extending these methods through deep learning.

In Chapter 2, we will provide an in-depth literature review on multiview learning and self-supervised learning techniques, establishing a foundational understanding of these methodologies in the context of biomedical data.

In Chapter 3, we propose a novel formulation for CCA, introducing an approach to solve it by gradient descent. This methodology allows for scalable solutions by enabling stochastic gradient descent, thus overcoming the limitations of traditional full-batch algorithms.

In Chapter 4, we introduce regularization via proximal gradient descent into our framework. This addition allows us to incorporate structured priors into our model, improving the interpretability of our findings, and enhancing their relevance to the biomedical context.

In Chapter 5, we show our approach can be extended to Deep CCA and Self-Supervised Learning more generally. While the sample sizes in biomedical research are still too small to apply these methods, we lay the groundwork for their potential incorporation into our framework in the future.

In Chapter 6, we present CCA-Zoo, a Python package for CCA and multiview learning, which implements the methodologies proposed in this thesis. We show how this work filled a gap in the Python ecostystem, and how it will facilitate the adoption of these methods in the biomedical community.

In Chapter 7, we discuss the implications, challenges, and limitations of our work, and how they can be addressed in future research.

The main contributions of this thesis are:

\begin{itemize}
    \item The introduction of a new formulation for CCA and generalized eigenvalue problems which can be solved via gradient descent, enabling the scaling of these methods to much larger datasets.
    \item The integration of regularization into the proposed framework using proximal gradient descent, enabling the inclusion of structured priors and improving the interpretability of the findings.
    \item Laying the groundwork for the potential incorporation of deep learning methods into the framework when the sample size permits, enabling the capture of non-linear associations in the data.
    \item
\end{itemize}

Through these contributions, this thesis aims to bridge the gap between the latent potential of large-scale, complex biomedical data and the capabilities of existing analytical methodologies.
In doing so, we take one step closer to truly personalized medicine.
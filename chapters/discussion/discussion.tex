\chapter{Conclusion}\label{ch:discussion}

This chapter provides a summary of the findings of this thesis, discusses their implications, and outlines potential directions for future work. 


\section{Summary of Contributions}

\subsection{Regularisation of CCA Models:
A Flexible Framework based on
Alternating Least Squares}
This chapter presented the FRALS framework for CCA, addressing challenges in analyzing large-scale neuroimaging datasets from projects such as the Human Connectome Project and the Alzheimer's Disease Neuroimaging Initiative. Incorporating structured priors through regularization, particularly the elastic net penalty, FRALS enhanced the interpretability and generalizability of CCA models. This method, documented in the work presented at the OHBM \citep{chapman2023als}, has been effective in uncovering significant brain-behavior associations, showing superior out-of-sample performance compared to traditional methods.

\subsection{Insights From Generating
Simulated Data for CCA}
This chapter contributed to the debate on the interpretation of model weights versus loadings in CCA. By generating high-dimensional simulated data and categorizing methods into explicit and implicit latent variable models, the chapter highlights the robustness of loadings to columnwise transformations in data matrices, a feature not shared with weights. The simulated data strategies formed part of the analysis in \citet{mihalik2022canonical} and influenced the analysis in \citet{ADAMS2024}.

\subsection{Efficient Algorithms for the
CCA Family: Unconstrained
Losses with Unbiased
Gradients}
Focusing on scaling challenges for CCA and PLS in the context of large-scale biomedical datasets like the UK Biobank, this chapter introduces a new gradient descent algorithm tailored for generalized eigenvalue problems. The methods developed, informed by publications \citep{chapman2022generalized, chapman2023efficient, chapman2023cca}, enable the application of multiview CCA and PLS to datasets with extensive dimensions and complex structures.

\subsection{Deep CCA and Self-Supervised
Learning}
This chapter introduces a novel formulation of Deep CCA optimized for the stochastic minibatch setting and proposes SSL-EY, a new competitive SSL method. Grounded in findings from \citep{chapman2023cca} and \citep{chapman2023efficient}, the chapter demonstrates the robustness of these methods against hyperparameter sensitivity and elucidates connections between CCA-based SSL methods and other contemporary SSL approaches.

\subsection{CCA-Zoo: A collection of
Regularized, Deep
Learning-based, Kernel, and
Probabilistic methods in a
scikit-learn style framework}
Presenting \texttt{CCA-Zoo}, a Python library that consolidates and enhances the accessibility of multiview learning methods, this chapter details the development and capabilities of the library, which implements a variety of CCA, PLS, and related techniques. As detailed in \citep{chapman2021cca}, \texttt{CCA-Zoo} addresses gaps in existing software offerings and facilitates broader adoption and innovation within the research community.

In summary, we have demonstrated novel ways to introduce structured priors into CCA models, developed efficient algorithms for large-scale CCA, extended CCA to deep learning, and provide a unified interface for various CCA methods. Finally, we have made software implementations of these methods available to the research community through the \texttt{CCA-Zoo} package which have already been well-received by the community.

\section{Future Work}

\subsection{Applications}

\subsubsection{Large-Scale Neuroimaging Datasets}

While the applications presented in this thesis, particularly the UK Biobank analysis in Chapter \ref{ch:deep_learning}, have demonstrated the potential of our methods, there is still vast untapped potential in applying these techniques to even larger and more diverse datasets. The ABCD dataset, for instance, offers a rich source of multimodal data that could benefit from the regularized and scalable CCA methods developed in this thesis. Preliminary results on this dataset have shown promise, and we believe that further exploration will yield valuable insights into brain development and its associated factors.

\subsubsection{Wearable Devices}

The rise of wearable devices and the proliferation of biometric data present new opportunities for applying multiview learning techniques to personal health monitoring. By integrating data streams from devices such as smartwatches, continuous glucose monitors, and sleep trackers, we can gain insights into an individual's physical and mental well-being that were previously inaccessible. I strongly believe that the development of interpretable and scalable methods for analyzing these diverse data sources will be crucial for unlocking the full potential of wearable technology in personalized healthcare.

\subsection{Methods}

\subsubsection{Proximal Gradient Descent for Regularized CCA}

Preliminary experiments suggest that the proximal gradient descent approach is much faster than existing methods, making it a promising direction for future research. We anticipate that this methodology will significantly enhance the scalability and applicability of CCA, PCA, and PLS in the era of big data and complex regularization schemes.



\section{Closing Remarks}

My PhD journey has been a fascinating exploration of the world of multiview learning, with Canonical Correlation Analysis (CCA) at its core. What began as a quest to apply deep learning to uncover brain-behavior associations quickly evolved into a multifaceted endeavor that led me to develop scalable algorithms for linear CCA and investigate the connections between CCA and self-supervised learning.

When I first embarked on this journey, I was eager to apply Deep CCA to high-dimensional neuroimaging data to gain insights into the complex relationship between the brain and mental health. However, I soon realized that existing Deep CCA methods were computationally infeasible for such datasets, and even moderately sized datasets could not fully capture the intricacies of this relationship.

Undeterred, I shifted my focus to developing efficient ways to regularize CCA models. This led me to confront the scalability bottleneck of CCA head-on. By developing efficient algorithms for CCA and Partial Least Squares (PLS), I was able to push the boundaries of what was possible with these methods. This exploration also led me to investigate the connections between CCA and self-supervised learning, bringing my journey full circle back to the realm of deep learning.

One of the highlights of my journey has been witnessing the rapid growth and evolution of multiview learning during my PhD. From the emergence of powerful multimodal language models to the increasing adoption of self-supervised learning techniques, it has been thrilling to be a part of this dynamic and fast-paced field.

Another highlight has been the impact of the software we developed, such as the CCA-Zoo package. Seeing researchers across various fields utilize our tools to tackle a wide range of problems has been immensely gratifying. It is a testament to the importance of developing accessible and efficient implementations of these methods.

As I reflect on this journey, I am filled with excitement for the future of multiview learning. The integration of deep learning with CCA and the application of these methods to ever-larger and more diverse datasets hold immense promise. I believe that the work presented in this thesis has laid a foundation for further advancements in this field, and I am eager to see how others will build upon it.

Thank you for reading.
\subsection{`EigenGame' Approach for Ordered Subspaces}\label{sec:gep-eg-formulation}

We now present a second formulation of the top-$K$ subspace of a GEP, which is more closely related to the `EigenGame' approach of \cite{gemp20,gemp2021}.

Our first proposed method solves the general form of the generalized eigenvalue problem in equation (\ref{eq:consgep}) for the top-k eigenvalues and their associated eigenvectors in parallel. We are thus interested in both the top-k subspace problem and the top-k eigenvectors themselves. Our method extends the Generalized Hebbian Algorithm to GEPs, and we thus refer to it as GHA-GEP.

In the full-batch version of our algorithm when $A$ is known to be positive semidefinite, each eigenvector estimate has updates with the form

\begin{align}
    \Delta_{i}^{\text{GHA-GEP-PSD}}
    &=
    \overbrace{A \hat{w}_{i}}^{\text{Reward}} - \overbrace{\sum_{j \leq i}B\hat{w}_{j}\textcolor{blue}{\left(\hat{w}_{j}^{\top} A \hat{w}_{i}\right)}}^{\text{Penalty}}
    &&=
    \overbrace{A \hat{w}_{i}}^{\text{Reward}} - \overbrace{B\hat{w}_{i}\textcolor{blue}{\left(\hat{w}_{i}^{\top} A \hat{w}_{i}\right)}}^{\text{Variance Penalty}} - \overbrace{\sum_{j < i}B\hat{w}_{j}\textcolor{blue}{\left(\hat{w}_{j}^{\top} A \hat{w}_{i}\right)}}^{\text{Orthogonality Penalty}} & \nonumber\\
    &=A \hat{w}_{i} - \sum_{j \leq i}B\hat{w}_{j}\textcolor{blue}{\Gamma_{ij}}
    &&=
    A \hat{w}_{i} - B\hat{w}_{i}\bGaminds{ii} - \sum_{j < i}B\hat{w}_{j}\bGaminds{ij}&
    \label{eq:psdupdate}
\end{align}

where $\hat{w_j}$ is our estimator to the eigenvector associated with the $j^{\text{th}}$ largest eigenvalue and in the stochastic setting, we can replace $A$ and $B$ with their unbiased estimates $\hat{A}$ and $\hat{B}$.
% We will use the notation $\textcolor{blue}{\Gamma_{ij}=\left(\hat{w}_{j}^{\top} A \hat{w}_{i}\right)}$ to highlight connections with previous work (Appendix \ref{sec:previousworkcomparison}) and illustrate its interpretation as a Lagrange multiplier in the optimization by analogy with previous work by \citet{chen2019constrained} in Appendix \ref{sec:chen}.
We will use the notation $\textcolor{blue}{\Gamma_{ij}=\left(\hat{w}_{j}^{\top} A \hat{w}_{i}\right)}$ to facilitate comparison with previous work in Appendix \ref{sec:previousworkcomparison}.  $\textcolor{blue}{\Gamma_{ij}}$ has a natural interpretation as Lagrange multiplier for the constraint $w_i^\top B w_j = 0$; indeed, \citet{chen2019constrained} prove that $\left(\hat{w}_{j}^{\top} A \hat{w}_{i}\right)$ is the optimal value of the corresponding Lagrange multiplier for their GEP formulation; we summarise this derivation in Appendix \ref{sec:chen} for ease of reference.
We also label the terms as rewards and penalties to facilitate discussion with respect to the EigenGame framework in Appendix \ref{sec:mueigengame} and recent work in self-supervised learning in Appendix \ref{sec:ssl}.

However, when $A$ has negative eigenvalues, the iteration defined in (\ref{eq:psdupdate}) can `blow-up' from certain initial values. We therefore propose the following modification:
\begin{equation}
    \label{eq:ourupdate}
    \Delta_{i}^{\text{GHA-GEP}}=
    A \hat{w}_{i} - B\hat{w}_{i}\,\textcolor{violet}{\max(}\bGaminds{ii}\textcolor{violet}{,0)} - \sum_{j < i}B\hat{w}_{j}\bGaminds{ij}
\end{equation}
Note that this reduces to (\ref{eq:psdupdate}) when $A$ is positive semi-definite. The following proposition, proved in Appendix \ref{app:gha-gep}, justifies this choice of update.

\begin{restatable}[Unique stable stationary point]{proprep}{gepghastationary}
    \label{prop:gep-gha-stat}
    Given exact parents and assuming the top-k generalized eigenvalues
    of $A$ and $B$ are distinct and positive, the only stable stationary point of the iteration defined by (\ref{eq:ourupdate}) is the eigenvector $w_i$ (up to sign).
\end{restatable}

\subsection{Defining Utilities and Pseudo-Utilities with Lagrangian Functions}

Now observe that the updates (\ref{eq:psdupdate}) can be written as the gradients of a Lagrangian pseudo-utility function:
\begin{align}
    \label{eq:lagrangeutils}
    \mathcal{PU}_{i}^{\text{GHA-GEP-PSD}}(w_i | w_{j<i}, \bGam )
    &=\tfrac{1}{2} \hat{w}_{i}^{\top}A\hat{w}_{i}
    +\tfrac{1}{2} \textcolor{blue}{\Gamma_{ii}} (1 - \hat{w}_{i}^{\top}B\hat{w}_{i})
    -\sum_{j< i} \textcolor{blue}{\Gamma_{ij}} \hat{w}_{j}^{\top}B\hat{w}_{i}.
\end{align}

We show how this result is closely related to the pseudo-utility functions in \citet{chen2019constrained} and suggests an alternative pseudo-utility function for the work in \citet{gemp2021} in Appendix \ref{sec:pseudo-utils} which, unlike the original work, does not require stop gradient operators.

If we plug in the relevant $w_i$ and $w_j$ terms into $\bGam$, we obtain the following utility function:
\begin{align}
    \label{eq:game-utility-psd}
    \mathcal{U}_{i}^{\delta\text{-PSD}}(w_i ; w_{j<i})
    &=\tfrac{1}{2}\hat{w}_{i}^{\top}A\hat{w}_{i}
    +\tfrac{1}{2}\hat{w}_{i}^{\top}A\hat{w}_{i}\left(1-\hat{w}_{i}^{\top}B\hat{w}_{i}\right)
    -\sum_{j< i} \hat{w}_{i}^{\top}A\hat{w}_{j} \hat{w}_{j}^{\top}B\hat{w}_{i} \nonumber \\
    &= (\hat{w}_{i}^{\top}A\hat{w}_{i})
    - \tfrac{1}{2} (\hat{w}_{i}^{\top}A\hat{w}_{i})(\hat{w}_{i}^{\top}B\hat{w}_{i})
    - \sum_{j< i} (\hat{w}_{i}^{\top}A\hat{w}_{j})( \hat{w}_{j}^{\top}B\hat{w}_{i})
\end{align}

Again, we apply a modification to prevent blow-up when $A$ has negative eigenvalues, giving utility
\begin{align}
    \label{eq:game-utility}
    \mathcal{U}_{i}^{\delta}(w_i ; w_{j<i})
    &= (\hat{w}_{i}^{\top}A\hat{w}_{i})
    - \tfrac{1}{2} \textcolor{violet}{\max(}(\hat{w}_{i}^{\top}A\hat{w}_{i})\textcolor{violet}{,0)}\,(\hat{w}_{i}^{\top}B\hat{w}_{i})
    - \sum_{j< i} (\hat{w}_{i}^{\top}A\hat{w}_{j})( \hat{w}_{j}^{\top}B\hat{w}_{i})
\end{align}

A remarkable fact is that this utility function actually defines a solution to the GEP problem! We prove the following consistency result in Appendix \ref{nashproof}.

\begin{restatable}[Unique utility maximiser]{proprep}{deltanash}
    \label{prop:delta-nash}
    Assuming the top-$i$ generalized eigenvalues of the GEP (\ref{eq:consgep}) are positive and distinct. Then the unique maximizer of the utility in (\ref{eq:game-utility}) for exact parents is precisely the $i^{th}$ eigenvector (up to sign).
\end{restatable}

This utility function allows us to formalise $\Delta$-EigenGame, whose solution corresponds to the top-k solution of equation (\ref{eq:consgep}).

\begin{definition}
    Let $\Delta$-EigenGame be the game with players $i \in \{1,...,k\}$, strategy space $\hat{w}_{i} \in \mathbb{R}^d$, where d is the dimensionality of A and B, and utilities $\mathcal{U}_{i}^{\delta}$ defined in equation (\ref{eq:game-utility}).
\end{definition}

An immediate corollary of Proposition \ref{prop:delta-nash} is:
\begin{corollary}
    The top-$k$ generalized eigenvectors form the unique, strict Nash equilibrium of $\Delta$-EigenGame.
\end{corollary}

Furthermore, the penalty terms in the utility function (\ref{eq:lagrangeutils}) have a natural interpretation as a projection deflation as shown in appendix \ref{sec:defl}.

Next note that it is easy to compute the derivative
\begin{align}
    \label{eq:utility-update}
    \Delta_{i}^\delta
    &=
    \frac{\partial \mathcal{U}_{i}^{\delta}(w_i ; w_{j<i})}{\partial w_i} \\
    &=
    2 A\hat{w}_{i}
    - \{ A\hat{w}_{i}(\hat{w}_{i}^{\top}B\hat{w}_{i}) + (\hat{w}_{i}^{\top}A\hat{w}_{i})B \hat{w}_{i}\}
    - \sum_{j< i} \{A\hat{w}_{j}(\hat{w}_{j}^{\top}B\hat{w}_{i}) + (\hat{w}_{j}^{\top}A\hat{w}_{i})B \hat{w}_{j}\}\nonumber\\
    &=
    \Delta_i^\text{GHA-GEP} + \{ A\hat{w_i} - \sum_{j\leq i} A\hat{w}_{j}(\hat{w}_{j}^{\top}B\hat{w}_{i})\} \nonumber
\end{align}
We can use these gradients as updates step for an alternative algorithm for the GEP which we call $\delta$-EigenGame (where, consistent with previous work, we use upper case for the game and lower case for its associated algorithm). We can now discuss stochastic versions of the algorithms introduced above, the setting where our methods excel.

\subsection{Stochastic/Data-streaming versions}

This paper is motivated by cases where the algorithm only has access to unbiased sample estimates of $A$ and $B$. These estimates, denoted $\hat{A}$ and $\hat{B}$, are therefore random variables. A nice property of both our proposed GHA-GEP and $\delta$-EigenGame is that $A$ and $B$ appear as multiplications in both of their updates (as opposed to as divisors). This means that we can simply substitute them for our unbiased estimates at each iteration. For the GHA-GEP algorithm this gives us updates based on stochastic unbiased estimates of the gradient

\subsection{Applications to (multi-view) stochastic CCA and PLS, and Deep CCA}
\begin{lemma}[Objective recovers GEP formulation of linear (multi-view) CCA]
    When the $f\sps{i}$ are linear, as in \cref{eq:cca-linear-function-def}, the population loss from \cref{eq:EY-loss-def-C-V} recovers MCCA as defined in \cref{sec:CCA-family}. %Moreover, the minimal loss value is precisely $-\norm{\MCCA_K(X)}$
\end{lemma}
\begin{proof}
    By construction, for linear MCCA we have $C = U^T A U,\, V_\alpha=U^T B_\alpha U$, where $(A, B_\alpha)$ define the GEP for MCCA introduced in \cref{eq:gep-most-general-formulation}.
    So $\LEY(U) = \LEYGEP(U)$ and by \cref{prop:EY-charac} the optimal set of weights define a top-$K$ subspace of the GEP, and so is a MCCA solution.
\end{proof}

Moreover, by following through the chain of back-propagation, we obtain gradient estimates in $\mathcal{O}(MKD)$ time.
Indeed, we can obtain gradients for the transformed variables in $\mathcal{O}(M K^2)$ time so the dominant cost is then updating $U$; we flesh this out with full details in \cref{supp:fast-updates}.

\begin{restatable}{lemma}{recoverDeepCCA}[Objective recovers Deep Multi-view CCA]
    \label{lem:recover-DeepCCA}
    Assume that there is a final linear layer in each neural network $f\sps{i}$.
    Then at any local optimum, $\hat{\theta}$, of the population problem, we have
    \begin{align*}
        \LEY(\hat{\theta}) = - \norm{\MCCA_K(\hat{Z})}_2^2
    \end{align*}
    where $\hat{Z} = f_{\hat{\theta}}(X)$.
    Therefore, $\hat{\theta}$ is also a local optimum of objectives from \cite{andrew2013deep, somandepalli2019multimodal} as defined in \cref{eq:DMCCA-def}.
\end{restatable}
\begin{proof}[Proof sketch: see \cref{supp:EY-recover-Deep-CCA} for full details.]
    Consider treating the penultimate-layer representations as fixed, and optimising over the weights in the final layer. This is precisely equivalent to optimising the Eckhart-Young loss for linear CCA where the input variables are the penultimate-layer representations. So by \cref{prop:no-spurious}, a local optimum is also a global optimum, and by \cref{prop:EY-charac} the optimal value is the negative sum of squared generalised eigenvalues.
\end{proof}
\section{\textcolor{red}{\textbf{Project C:} Random Sample Algorithms for Sample Selection and Dimensionality Reduction}}

\subsection{\textcolor{red}{Repeated Random Sample PCA}}

Inspired by elements of the SVM-like form of CCA, we develop an algorithm for simultaneous sample selection and PCA. The idea is to repeatedly fit a PCA model on a subset of the data. We then apply this model to the whole data and record the scores of each sample. We can then select samples which have a high variance (above a threshold) in a large number of the model iterations defined by some parameter $p$.

\vspace{\baselineskip}
\begin{algorithm}
\begin{algorithmic}[1]
\STATE {Finds regression model parameters which best fit the data}
\STATE {$k=$ maximum iterations}
\STATE {$n=$ minimum number of samples in each fit, typically the minimum number required by the model e.g. the number of parameters for regression}
\STATE {$t=$ threshold for point to be considered well fit}
\WHILE {iterations $< k$}
    \STATE Randomly assign instances to the $k$ clusters.
    \STATE For $i=1 \cdots k$, apply CCA to cluster $i$ to build $M^{i}=\left\{\left(u_{j}, v_{j}\right), r_{j},\left(a_{j}, b_{j}\right): j=1 \cdots d\right\}$, i.e., the top $d$ pairs of canonical variates, the correlation $r$ between each pair, and the corresponding $d$ pairs of projection vectors.
    \STATE Reassign each instance to a cluster based on its $\vec{x}$ and $\vec{y}$ features and the $k$ CCA models.
    \STATE If no assignment has changed from previous iteration, return the current clusters and CCA models. Otherwise, go to step 2 .
\ENDWHILE
\caption[Repeated Random Sample PCA]{Repeated Random Sample PCA}
\label{Repeated Random Sample PCA}
\end{algorithmic}
\end{algorithm}

\subsection{\textcolor{red}{Repeated Random Sample CCA}}

Extending the repeated PCA model, we apply a similar idea to the CCA and PLS problems. At each iteration, the algorithm fits a PLS or CCA model to a random subset of the data. We then apply the fit model to all of the data and store which projections had a product greater than some threshold $t$ ($t=0$ provides a natural definition for positive correlation and greater would make the model more robust by requiring large products). At the end of the algorithm we select samples that were above the threshold in lots of repeats defined by some parameter $p$. Finally, we fit a model to these points and return the classification.

\vspace{\baselineskip}
\begin{algorithm}
\begin{algorithmic}[1]
\STATE {Finds regression model parameters which best fit the data}
\STATE {$k=$ maximum iterations}
\STATE {$n=$ minimum number of samples in each fit, typically the minimum number required by the model e.g. the number of parameters for regression}
\STATE {$t=$ threshold for point to be considered well fit}
\WHILE {iterations $< k$}
    \STATE Randomly assign instances to the $k$ clusters.
    \STATE For $i=1 \cdots k$, apply CCA to cluster $i$ to build $M^{i}=\left\{\left(u_{j}, v_{j}\right), r_{j},\left(a_{j}, b_{j}\right): j=1 \cdots d\right\}$, i.e., the top $d$ pairs of canonical variates, the correlation $r$ between each pair, and the corresponding $d$ pairs of projection vectors.
    \STATE Reassign each instance to a cluster based on its $\vec{x}$ and $\vec{y}$ features and the $k$ CCA models.
    \STATE If no assignment has changed from previous iteration, return the current clusters and CCA models. Otherwise, go to step 2 .
\ENDWHILE
\caption[Repeated Random Sample CCA]{Repeated Random Sample CCA}
\label{alg:Repeated Random Sample CCA}
\end{algorithmic}
\end{algorithm}
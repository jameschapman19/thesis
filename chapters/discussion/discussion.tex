\chapter{Conclusion}\label{ch:discussion}

This chapter provides a summary of the findings of this thesis, discusses their implications, and outlines potential directions for future work. 

\subsection{Regularisation of CCA Models:
A Flexible Framework based on
Alternating Least Squares}
This chapter presented the FRALS framework for CCA, addressing challenges in analyzing large-scale neuroimaging datasets from projects such as the Human Connectome Project and the Alzheimer's Disease Neuroimaging Initiative. Incorporating structured priors through regularization, particularly the elastic net penalty, FRALS enhanced the interpretability and generalizability of CCA models. This method, documented in the work presented at the OHBM \citep{chapman2023als}, has been effective in uncovering significant brain-behavior associations, showing superior out-of-sample performance compared to traditional methods.

\subsection{Insights From Generating
Simulated Data for CCA}
This chapter contributed to the debate on the interpretation of model weights versus loadings in CCA. By generating high-dimensional simulated data and categorizing methods into explicit and implicit latent variable models, the chapter highlights the robustness of loadings to columnwise transformations in data matrices, a feature not shared with weights. The simulated data strategies formed part of the analysis in \citet{mihalik2022canonical} and influenced the analysis in \citet{ADAMS2024}.

\subsection{Efficient Algorithms for the
CCA Family: Unconstrained
Losses with Unbiased
Gradients}
Focusing on scaling challenges for CCA and PLS in the context of large-scale biomedical datasets like the UK Biobank, this chapter introduces a new gradient descent algorithm tailored for generalized eigenvalue problems. The methods developed, informed by publications \citep{chapman2022generalized, chapman2023efficient, chapman2023cca}, enable the application of multiview CCA and PLS to datasets with extensive dimensions and complex structures.

\subsection{Deep CCA and Self-Supervised
Learning}
This chapter introduces a novel formulation of Deep CCA optimized for the stochastic minibatch setting and proposes SSL-EY, a new competitive SSL method. Grounded in findings from \citep{chapman2023cca} and \citep{chapman2023efficient}, the chapter demonstrates the robustness of these methods against hyperparameter sensitivity and elucidates connections between CCA-based SSL methods and other contemporary SSL approaches.

\subsection{CCA-Zoo: A collection of
Regularized, Deep
Learning-based, Kernel, and
Probabilistic methods in a
scikit-learn style framework}
Presenting \texttt{CCA-Zoo}, a Python library that consolidates and enhances the accessibility of multiview learning methods, this chapter details the development and capabilities of the library, which implements a variety of CCA, PLS, and related techniques. As detailed in \citep{chapman2021cca}, \texttt{CCA-Zoo} addresses gaps in existing software offerings and facilitates broader adoption and innovation within the research community.

\section{Future Work}

\subsection{Applications}

One of the most rewarding aspects of this thesis has been seeing the software we developed, such as the CCA-Zoo package, being used by researchers across different fields to tackle a wide range of problems. It has been truly inspiring to witness the impact of our work and to see how it has enabled others to push the boundaries of multiview learning.

While the applications presented in this thesis, particularly the UK Biobank analysis in Chapter \ref{ch:deep_learning}, have demonstrated the potential of our methods, there is still vast untapped potential in applying these techniques to even larger and more diverse datasets. The ABCD dataset, for instance, offers a rich source of multimodal data that could benefit from the regularized and scalable CCA methods developed in this thesis. Preliminary results on this dataset have shown promise, and we believe that further exploration will yield valuable insights into brain development and its associated factors.

Moreover, the integration of non-imaging modalities, such as Electronic Health Records and audio transcripts, with neuroimaging data presents an exciting avenue for future research. The success of transformer architectures in natural language processing suggests that these methods could be powerful tools for analyzing and integrating textual and auditory data with brain imaging. By leveraging the capabilities of deep learning and the multiview learning framework of CCA, we can uncover complex relationships across modalities and gain a more comprehensive understanding of human health and behavior.

\subsection{Methods}

Proximal gradient descent

Better theory for Self Supervised Learning.

\section{Closing Remarks}

As I reflect on my journey from struggling with the concepts of eigenvalues and eigenvectors as an undergraduate to delving into the intricacies of CCA as a doctoral researcher, I am struck by the depth and breadth of this field. Far from being a narrow topic, CCA has served as a gateway to explore a wide range of subjects, from optimization and Bayesian statistics to deep learning and software engineering.

One of the most exciting aspects of this journey has been witnessing the rapid growth and evolution of multiview and self-supervised learning during my PhD. The convergence of these fields with CCA has been truly remarkable, and I am thrilled to have been a part of this exciting development. The methods and insights gained from this thesis have the potential to contribute to the continued growth and impact of these areas.

Although we have been limited by access to data, I am confident that the work presented in this thesis has made a meaningful impact on the field of multiview learning. The software tools we have developed, such as CCA-Zoo, have already been adopted by researchers across various domains, enabling them to push the boundaries of what is possible with CCA and related methods.

With its rich history dating back to the 1930s, CCA has been a powerful tool for discovering new associations across various domains. The work presented in this thesis aims to contribute to the continued relevance and applicability of CCA in the era of big data and deep learning. By developing interpretable, regularized, and scalable methods, we have sought to unlock the full potential of CCA and multiview learning.

As we look to the future, I am excited by the prospects of further advancements in this field. The integration of deep learning with CCA, the application of these methods to ever-larger and more diverse datasets, and the potential for uncovering groundbreaking insights all point to a bright future for multiview learning. I hope that this thesis has not only made valuable contributions to the field but also inspired others to explore the fascinating world of CCA and its applications.

Thank you for reading.
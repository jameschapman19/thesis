\chapter{Flexible, Scalable Regularization for Generalized Eigenvalue Problems for Multiview Subspace Learning}
\label{Regularised}

The content relating to regularised alternating least squares as a method for optimizing regularised CCA is based on an abstract which I presented in poster form at OHBM. The work benefitted from extensive theoretical discussions with John Shawe-Taylor and Janaina Mourao-Miranda as well as Janaina and Rick Adams' help interpreting the results.

\section{Introduction}


\section{Background and Related Work}

\section{Related Work}

In this section we review some existing regularised CCA methods including some of the existing extensions to data with more than two views. Since almost all of the existing work considers sparsity inducing regularisation this is the main focus.

There have been many different proposals in the literature for finding sparse regularised solutions. In this section, we describe the penalized matrix decomposition method (arguably the most common in applied sparse CCA) as well as some of the methods based on least squares formulations. Like the unregularised CCA and PLS formulations, they can broadly be grouped into penalized SVD based methods and penalised iterative methods.

\subsection{Penalized Matrix Decomposition}\label{sec:witten}

Witten's "Penalized Matrix Decomposition" (PMD) \cite{witten2009penalized} approach substitutes the CCA constraints $\bold{w_i^{\top}X_i^{\top}X_iw_i}=1$ for the PLS constraints $\bold{w_i^{\top}w_i}=1$. This is equivalent to assuming that the variance-covariance matrix of $X$ is an identity matrix or in other words that the columns of the original data matrices are independent. This subtle change means that Witten's method introduced in this paper is only an approximation to a sparse CCA optimisation problem. It follows that this method will perform best (in terms of optimising the `true' CCA objective) when the identity covariance matrix assumption is strongest. In addition, much as with PLS, PMD is biased towards the largest principal components in the data and will select directions with high covariance even if they are not the maximally correlated directions.

Since the PLS constraint $\bold{w_1^{\top}w_1}=1$ is non-convex, Witten further relaxes this constraint to an inequality. The penalized matrix decomposition can then be expressed as the constrained optimization problem in \ref{eq:pmd}. 

\begin{align}
    \label{eq:pmd}
    & \bold{w_{opt}}=\underset{\bold{w}}{\mathrm{argmax}}\{ \bold{w_1^{\top}X_1^{\top}X_2w_2} \}\\
    & \text{subject to:} \notag\\
    & \bold{w_1^{\top}w_1}\leq1 \notag\\
    & \bold{w_2^{\top}w_2}\leq1 \notag\\
    & P(\bold{w_1})\leq c_1 \notag\\
    &P(\bold{w_2})\leq c_2 \notag
\end{align}

Where $P(\bold{w_1})=\|\bold{w_1}\|_1$ would form a constraint on the 1-norm but in principle the algorithm only requires that $P(\bold{w_1})$ is a one-homogenous function:

\begin{align}
    & \lambda P(\bold{w})=P(\lambda \bold{w})
\end{align}

In order to solve this optimisation, Witten proposed an algorithm based on a rank 1 singular value decomposition of a matrix $\bold{M}=\bold{X_1^{\top}X_2}$. Since soft-thresholding is equivalent to Lasso when $\bold{X_i^{\top}X_i}=\bold{I}$, the algorithm is simply the power method for singular value decomposition with soft-thresholding applied at each iteration such that $\|\bold{w_i}\| \leq c_i$. The soft-threshold function can be written as:

\begin{align}
    \label{eq:softthreshold}
    & S(x,\gamma) = \text{sign}(x)\text{max}(0,\|\bold{x}\|-\gamma)
\end{align}

Where $\gamma$ defines the threshold of the absolute value below which coefficients in $x$ are set to zero. We can then iteratively find the threshold $\gamma$ that fulfils the l1-norm condition using for example binary search:

\vspace{\baselineskip}
\begin{algorithm}[H]
\begin{algorithmic}
\STATE {Finds local optima for sparse left and right eigenvectors of the covariance matrix} $X_1^{\top}X_2$
\STATE $M_1=X_1^{\top}X_2$
\FOR {$k \gets 1$ \textbf{to} $K$}
    \STATE Initialize $|w^{(k)}_2|_2=1$
    \WHILE {Not converged}
        \STATE $w^{(k+1)}_1=M_kw^{(k)}_2$
        \STATE $w^{(k+1)}_1=S(w^{(k+1)}_1,\gamma)$
        \STATE $w^{(k+1)}_1=\frac{w^{(k+1)}_1}{\|w^{(k+1)}_1\|_2}$
        \STATE $w^{(k+1)}_2=M_k^{\top}w^{(k+1)}_1$
        \STATE $w^{(k+1)}_2=S(w^{(k+1)}_1,\gamma)$
        \STATE $w^{(k+1)}_2=\frac{w^{(k+1)}_2}{\|w^{(k+1)}_2\|_2}$
        \STATE $M_{k+1}=M_k-du_kv_k^{\top}$
    \ENDWHILE
\ENDFOR
\caption[Penalized Matrix Decomposition]{Penalized Matrix Decomposition for CCA}
\end{algorithmic}
\end{algorithm}
\vspace{\baselineskip}

Witten also showed that the lasso constraint could be replaced with a fused lasso constraint by replacing the soft-threshold with an appropriate update step.

\subsection{Penalized CCA}

Parkhomenko proposed a similar algorithm to penalized matrix decomposition, substituting the covariance matrix $\bold{X_1^{\top}X_2}$ for the correlation matrix $\bold{(X_1X_1^{\top})^{-\frac{1}{2}}X_1^{\top}X_2(X_2^{\top}X_2)^{-\frac{1}{2}}}$\cite{parkhomenko2009sparse}. However their soft-thresholding step uses the threshold as the hyper-parameter and therefore cannot be written as a constrained optimization problem. For this reason it is also difficult to select an appropriate range for the hyperparameters $\gamma_i$ as this is more dataset dependent:

\begin{align}
    \label{eq:parkho}
    & \bold{w_{opt}}=\underset{\bold{w}}{\mathrm{argmax}}\{ \bold{(X_1X_1^{\top})^{-\frac{1}{2}}X_1^{\top}X_2(X_2^{\top}X_2)^{-\frac{1}{2}}} \}
\end{align}

While this makes the optimisation more CCA-like than PLS-like, it requires us to calculate the inverses $\bold{(X_1X_1^{\top})^{-\frac{1}{2}}}$ and $\bold{(X_2^{\top}X_2)^{-\frac{1}{2}}}$. However in cases where the number of features $p$ is greater than the number of samples $n$, as is common in many applications of sparse modelling such as neuroimaging, these matrices cannot be inverted. 

For this reason, Parkhomenko recommended diagonalising the covariance matrices $\bold{X_i^{\top}X_i}$ or making them identity matrices. Witten's method can thus be seen as an extremely regularised version of Parkhomenko's. 

\subsubsection{Alternating Least Squares}\label{sec:ALS}

Since the scale invariance constraints in the CCA optimisation problem can also be represented by the unconstrained objective: 

\begin{align}
    & \bold{w_{opt}}=\underset{\bold{w}}{\mathrm{argmax}}\{\bold{\frac{\bold{w_1^{T}X_1^{T}X_2w_2}}{\|\bold{X_1w_1}\|_{2}\|\bold{X_2w_2}\|_{2}}}\}
\end{align}

And since by adding constants to the objective we can show that maximising the objective is equivalent to minimising the distance between the two normalized projections \cite{golub1995canonical}:

\begin{align}
    \left\|\bold{\frac{X_1 w_1}{\|X_1 w_1\|_{2}}-\frac{X_2 w_2}{\|X_2 w_2\|_{2}}}\right\|_{2}^{2}=2\left(1-\bold{\frac{w_2^{T} X_2^{T} X_1 w_1}{\|X_2 w_2\|_{2}\|X_1 w_1\|_{2}}}\right)
\end{align}

We can transform the problem of maximising correlation to one of minimising the distance between the normalized projections $\bold{X_1w_1}$ and $\bold{X_2w_2}$.

This optimisation problem is convex in one variable when the other variable is fixed. Problems of this kind are referred to as biconvex (in the two variable case) and multiconvex (where it is true for more than two variables). They are typically optimized by iterating between these two convex problems, successively fixing $\bold{w_1}$ and solving for $\bold{w_2}$ and vice versa. This is referred to in this case as the "alternating least squares" method \cite{lykou2010sparse}. 

It is known that the alternating least squares algorithm is slower to converge when the 1st and 2nd canonical correlations (singular values of the covariance matrix $\bold{X_1^{\top}X_2}$) are closer together \cite{venkatg}.

\subsection{Iterative Penalized Least Squares for Sparse CCA}

There have been some proposals for sparse CCA that relate to the alternating regression form of CCA. Wilms originally proposed "sparse alternating regressions" \cite{wilms2015sparse} which replace the least squares problems at each iteration with lasso penalized regressions and a normalization step as shown in equation \ref{eq:SAR}:

\begin{align}\label{eq:SAR}
    & \bold{\hat{w}^{(i+1)}_1}=\underset{\bold{w}}{\mathrm{argmin}}\{ \|\bold{Xw-y}\|^2_2 + \lambda\|\bold{w}\| \}\\
    & \bold{w^{(i+1)}_1}=\bold{\frac{\hat{w}^{(i+1)}_1}{\|\hat{w}^{(i+1)}_1\|}}
\end{align}

It is clear that when the regularisation parameters $\lambda$ are set to zero we recover the alternating least squares CCA formulation. 

Mai offered a proof that the sparse alternating regression normalization step gave a true global solution to a projection-length constrained lasso regression\cite{mai2019iterative}:

\begin{align}
    & \bold{w}=\underset{\bold{w}}{\mathrm{argmin}}\{ \|\bold{Xw-y}\|^2_2 + P(\bold{w}) \}\\
    & \text{subject to:} \notag\\
    & \bold{w^{\top}X^{\top}Xw}=1 \notag
\end{align}

Furthermore, Mai showed that in principle the lasso penalty could be replaced with any one homogenous regularisation functional. The proof is sketched out as below:

\begin{thm}

For one-homogenous function $P(x)$, if the solution $\bold{\hat{w}}$ satisfies the unconstrained problem:

\begin{align}
    & \bold{\hat{w}}=\underset{\bold{w}}{\mathrm{argmin}}\{ \|\bold{Xw-y}\|^2_2 + P(\bold{w}) \}
\end{align}

Then the solution $\bold{w}$ to the constrained problem: 

\begin{align}
    & \bold{w}=\underset{\bold{w}}{\mathrm{argmin}}\{ \|\bold{Xw-y}\|^2_2 + P(\bold{w}) \}\\
    & \text{subject to:} \notag\\
    & \bold{w^{\top}X^{\top}Xw}=1 \notag
\end{align}

is equal to $\bold{w}=\bold{\frac{\hat{w}}{\|X\hat{w}\|}}$ or $\bold{w}=\bold{(\hat{w}^{\top}X^{\top}X\hat{w})^{-\frac{1}{2}}\hat{w}}$

\end{thm}

\begin{proof}

We have $c^2=\bold{\hat{w}^{\top}X^{\top}X\hat{w}}$. By definition we must also have $1=(c^{-1}\bold{\hat{w}^{\top})X^{\top}X(c^{-1}\hat{w})}$. Mai notes that the solution $\bold{\hat{w}}$ must also satisfy a constrained optimisation problem:

\begin{align}
    & \bold{\hat{w}}= \underset{\bold{w}}{\mathrm{argmin}}\{ \|\bold{Xw-y}\|^2_2 + P(\bold{w}) \}\\
    & \text{subject to:} \notag\\
    & \bold{w^{\top}X^{\top}Xw}=c \notag
\end{align}

By substituting the constraints, Mai shows that this is the same as finding:

\begin{align}
    & \bold{\hat{w}}=\underset{\bold{w}}{\mathrm{argmin}}\{ -\bold{y^{\top}Xw} + P(\bold{w)} \}\\
    & \text{subject to:} \notag\\
    & \bold{w^{\top}X^{\top}Xw}=c \notag
\end{align}

Where $c=1$ for the unit length constrained problem. Mai then defines:

\begin{align}
    & J(\beta)=-\bold{y^{\top}Xw} + P(\bold{w})\\
\end{align}

and uses the chain of inequalities:

\begin{align}\label{eq:maiinequalitychain}
    & J(\bold{w}) \leq J(c^{-1}\bold{\hat{w}})=c^{-1}J(\bold{\hat{w}}) \leq c^{-1}J(c^{-1}\bold{w}) = J(\bold{w})
\end{align}

To show that $J(c^{-1}\hat{\beta})=J(\beta)$. 

\end{proof}

\subsection{Elastic Penalized CCA}

In neuroimaging applications, we typically encounter highly correlated variables in the raw data. Since the sparse methods discussed so far may only select one of a number of highly correlated variables, a well known property of the lasso penalty, we may lose out on parts of the interpretability of the model. Waiijenborg proposed the use of elastic net regularisation in order to ensure group-level sparsity using the updates \cite{waaijenborg2008quantifying}:

\begin{align}
    & \bold{\hat{w}^{(i+1)}_1}=\underset{\bold{w_1}}{\mathrm{argmin}}\{(1+\lambda^{(1)}_2)\|\bold{X_1w_1-X_2w^{(i)}_2}\|^2_2 + \lambda^{(1)}_2\|\bold{w_1}\|^2_2 + \lambda^{(1)}_1\|\bold{w_1}\|_1 \}\\
    & \bold{\hat{w}^{(i+1)}_2}=\underset{\bold{w_2}}{\mathrm{argmin}}\{(1+\lambda^{(2)}_2)\|\bold{X_1w_1^{(i)}-X_2w_2}\|^2_2 + \lambda^{(2)}_2\|\bold{w_2}\|^2_2 + \lambda^{(2)}_1\|\bold{w_2}\|_1 \}
\end{align}

Where $\bold{\hat{w}}$ denotes unnormalized weights which are normalized in the same way as \ref{eq:SAR}. While the algorithm produces good results in practice, it has no theoretical guarantee of convergence or optimality and cannot simply be written as an optimization problem. In particular, as we showed in the previous section, normalizing the solutions to the unconstrained update equations in order to fulfil the projection-length constraint does not guarantee that we find a global optimum for the constrained optimization. 


\section{Contributions}

\subsection{Simulated Data for Comparing Sparse CCA Models}

A necessary 

\subsection{Flexible Regularised ALS}

We can solve CCA by alternating minimisation over each view, based on the alternating least squares form. This form finds a variable $\bold{T}$ that is close to the latent variables $\bold{X}_i\bold{W}_i$, where $\bold{X}_i$, $\bold{W}_i$ are the matrix and weights for each view $i$. The closer $\bold{T}$ is to $\bold{X}_i\bold{W}_i$, the higher the correlation between them. The constraint  $\bold{T^{\top}T}=\bold{I}$ ensures that the latent space is orthogonal to find different effects.

\[ \underset{\bold{W},\bold{T}}{\mathrm{argmin}}\left\{\sum_i \|\bold{X}_i\bold{W}_i-\bold{T}\|_{F}^2 \right\} \]
  \[ \text{subject to: } \bold{T^{\top}T}=\bold{I} \]

To regularise the projection matrices, we add penalty terms to the objective function, such as $P(\bold{W}_i)=\lambda_i\|\bold{W}_i\|_F$ for ridge regression or $P(\bold{W}_i)=\lambda_i\|\bold{W}_i\|_1$ for lasso. This can help us avoid overfitting and improve the interpretability of the results. This means that \textbf{any regularised least squares solver} can be used to solve each subproblem, such as ridge regression, lasso, elastic net, etc. making our framework substantially more flexible than prior work.

\[ \underset{\bold{W},\bold{T}}{\mathrm{argmin}}\left\{\sum_i \|\bold{X}_i\bold{W}_i-\bold{T}\|_{F}^2 + \textcolor{red}{\lambda_i P(\bold{W}_i)}\}\right\} \]
  \[ \text{subject to: } \bold{T^{\top}T}=\bold{I} \]

A benefit of this approach is that neuroimaging practitioners may have existing code for implementing specific regularisation methods in a regression context which can be reused. Furthermore, they may have large dimensions both in features and increasingly in number of samples which benefit from using highly optimised software. 

\subsection{Flexible Regularised GEPs by Proximal Gradient Descent}

\section{Experiments: Flexible Regularised ALS for Brain-Behaviour Associations in the Human Connectome Project}

\subsection{Experiment Design}

Using FRALS, we solve a CCA problem using alternating elastic net regressions with tuned l1 and l2 regularization. We compare our proposed framework to prior work including ridge Regularised CCA (RCCA), Penalized Matrix Decomposition (PMD), commonly referred to in the literature as Sparse CCA but which strictly optimises covariance rather than correlation, and separate Principal Components Analysis (PCA) of the brain and behavioural data. We show that our method improves on RCCA by finding sparse solutions and improves on PMD by optimising a correlation rather than covariance based objective.

We used resting-state fMRI and non-imaging subject measures of 1003 healthy subjects from the 1200-subject data release of the Human Connectome Project (HCP). Following the main steps from \cite{smith2015positive}, we processed the rs-fMRI data into brain connectivity matrices. Like \cite{smith2015positive} we used 145 items of the non-imaging subject measures and removed the same confounding variables. We split the data into 80\% train and 20\% test and used cross-validation within the training data in order to tune hyperparameters.

\subsection{Results}

\subsubsection{Out of sample generalization}

\subsubsection{Model Similarities}

\subsubsection{Brain Loadings}

\subsubsection{Behaviour Loadings}

\section{Experiments: Total Variation Regularisation using Simulated Data}
\subsection{Experiment Design}
\subsection{Results}


\section{Experiments: Total Variation Regularisation of Brain-Behaviour Assocations in the ADNI Data}
\subsection{Experiment Design}
\subsection{Results}

\section{Comparison of Sparse CCA Methods in Simulated Data}
\subsection{Experiment Design}
\subsection{Results}


\section{Discussion and Conclusion}



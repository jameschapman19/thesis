\graphicspath{{chapters/loadings/}}


\chapter{Insights From Generating Simulated Data for CCA}\label{ch:loadings}
% \epigraph{All models are wrong, some are useful.}{\textit{G. Box}}
\minitoc
% chktex-file 44
% chktex-file 3
\section*{Preface}

This chapter, deriving insights from various projects, lays out both my arguments for the use of loadings in the interpretation of CCA models and a number of computational tricks that we used to generate simulated data with significantly higher dimensions than have been previously considered in the literature.
The simulated data generation methods were used to generate simulated data in \citet{mihalik2022canonical}.
The arguments for the use of loadings influenced our choice of loadings for the interpretation of the results in \citet{ADAMS2024}.

\section{Introduction}

Despite its popularity, there is an ongoing debate in the CCA literature regarding the interpretation of model weights versus loadings \citep{gu2018simultaneous}. This chapter aims to contribute to this debate by providing mathematical insights from generative models of CCA and empirical results from simulated data with higher dimensionality than previously considered in the literature.

We begin by categorizing methods for generating CCA simulated data into explicit and implicit latent variable models. This categorization allows us to compare and contrast the generative models in CCA literature with the generative model for linear regression. We highlight that in linear regression, regularization can be interpreted as a prior on the weights, whereas in CCA, it is perhaps more natural to interpret regularization as a prior on the loadings. By leveraging computational tricks, we demonstrate how to generate simulated data with significantly higher dimensions than previously considered in the literature \citep{helmer2020stability, matkovivc2023static}.

Furthermore, we rigorously prove that loadings are invariant to columnwise transformations in data matrices, unlike weights. This property makes CCA unique compared to Principal Component Analysis (PCA) or Partial Least Squares (PLS) and is particularly relevant in fields like brain-behavior studies, where data preprocessing often involves columnwise manipulation.

Our experimental design focuses on two main aspects. First, we evaluate the ability of CCA models to accurately recover the true model weights and loadings. Second, we examine the out-of-sample performance, which is often observed to be poor in practical datasets despite statistical significance, particularly for PLS-based models. This observation led us to question whether the issue lies in poor model fit or a lack of signal in the data with weak or biologically spurious correlations.

One of our most striking findings, consistent with the previous chapter, is the efficacy of Ridge Regularized CCA models compared to PLS models in identifying high correlations under anisotropic noise conditions. This complements earlier work \citep{helmer2020stability} that found that the number of samples needed to find high correlations increases with dimensionality; our results suggest that the important variable is the dimensionality of the smaller view.

Through this chapter, we aim to provide a comprehensive understanding of the relationship between weights and loadings in CCA models, the impact of regularization on model interpretation, and the performance of CCA models in high-dimensional settings. By unifying generative perspectives, proving mathematical properties, and conducting extensive simulations, we contribute to the ongoing debate in the CCA literature and provide valuable insights for researchers and practitioners applying CCA in various domains.

\section{Background: Weights and Loadings in Canonical Correlation Analysis}

As discussed in chapter \ref{ch:background}, Canonical Correlation Analysis (CCA) is a powerful multivariate statistical technique that explores the relationships between two sets of variables. It can be interpreted in two ways: either as a method that finds linear combinations of variables in two datasets that exhibit the highest correlation, or as a technique that estimates latent variables that are maximally correlated.

The concept of latent variables is particularly important in biomedical applications, as it can help uncover underlying factors influencing observable data. For example, in brain-behavior studies, latent variables may represent hidden neurological or cognitive processes that drive the relationship between brain structure or function and behavioral outcomes. Similarly, in imaging-genetics, latent variables can capture the genetic factors that influence brain morphology or activity patterns. By introducing latent variables, CCA enables researchers to gain a deeper understanding of complex phenomena like gene expression, pathologies, and normative variations in health-related data \citep{lawry2023multi}.

CCA's practical application revolves around two main approaches: the discriminative approach and the generative approach. The discriminative approach, represented as the `backward model' in Figure~\ref{fig:forward-backward-models}, uses weights to estimate highly correlated latent variables from observed data. It focuses on modeling the conditional distribution of the latent variables given the observed data, denoted as $P(Z|X\sps{1},X\sps{2})$. In contrast, the generative approach, known as the `forward model', emphasizes the data generation process and employs loadings to describe the relationship between latent variables and observed data. It models the joint distribution of the observed data conditioned on the latent variables, expressed as $P(X\sps{1},X\sps{2}|Z)$.

\begin{figure}
    \centering
    \tikz{
    % nodes
        \node[latent, align=center, minimum size=2cm] (Z) {Severity\\z};
        \node[obs, below left=of Z, minimum size=2cm, align=center] (x1) {Brain\\$x^{(1)}$};
        \node[obs, below right=of Z, minimum size=2cm, align=center] (x2) {Behaviour\\$x^{(2)}$};
        % edges
        \edge[blue]{Z} {x1};
        \edge[blue]{Z} {x2};
        \edge[red, tension=0.1]{x1} {Z};
        \edge[red, tension=0.1]{x2} {Z};
    }
    \caption[Forward and Backward Multiview Models]{\textit{\textbf{Forward and Backward Multiview Models:}} The \textcolor{blue}{generative/forward} and \textcolor{red}{discriminative/backward} approaches in \acrshort{cca}.}\label{fig:forward-backward-models}
\end{figure}

The distinction between the generative and discriminative approaches in CCA is analogous to the different interpretations of Principal Component Analysis (PCA) \citep{park2023critical}. PCA can also be viewed from a generative perspective, where the observed data are assumed to be generated from latent variables \citep{tipping1999probabilistic}, or from a discriminative perspective, where the principal components are linear combinations of the observed variables that maximize the variance \citep{hotelling1933analysis}.

In CCA research, there is an ongoing debate regarding the interpretation of models in terms of weights or loadings \citep{gu2018simultaneous}. Weights are often preferred for prediction tasks, as they directly relate the observed variables to the latent variables. On the other hand, loadings are favored for interpretation, as they provide insights into the structure and relationships within the data \citep{liu2022improved}. This discussion is particularly relevant to our work in chapter \ref{ch:als} and various studies involving sparse CCA and sparse Partial Least Squares (PLS), where understanding the meaning and implications of sparse loadings and weights is crucial.

Given the importance of this topic, especially in the context of our work in chapter \ref{ch:als} and other studies employing variants of sparse CCA and sparse PLS, it is essential to delve deeper into the interpretation of sparse loadings and weights. In the following sections, we will explore the mathematical properties and practical implications of weights and loadings in CCA, with a focus on sparse and regularized models. By addressing this key aspect of CCA, we aim to contribute to the ongoing debate and provide insights that can guide the application and interpretation of CCA in various domains, including neuroimaging, genetics, and health-related research.

\section{Unifying Generative Perspectives in CCA: Explicit and Implicit Latent Variable Models}

This section categorizes the generative models in CCA literature into explicit and implicit latent variable types, each offering distinct insights into the data generation process and the relationship between weights and loadings.

We will use some additional notational convention to describe probabilistic models.
We will use lowercase letters to represent samples from a distribution, and uppercase letters to represent random variables.
For example, \(x\) represents a sample from the distribution \(P(X)\), and \(X\) represents the random variable \(X\).

\subsection{Explicit Latent Variable Models: Probabilistic CCA and GFA}

In explicit latent variable models, we assume each view is generated from a linear model with added noise, conditional on the latent variable.
The distributions for the two views are given by:

\begin{align}
    z &\sim \mathcal{N}(0, I)\\
    x\sps{i} &\sim \mathcal{N}(W\sps{i} z + \mu\sps{i}, \Psi\sps{1})
\end{align}

Where \(z\) represents a sample from the latent guassian distribution, \(x\sps{i}\) represents a sample from the \(i\)-th view, \(W\sps{i}\) represents the model loadings, \(\mu\sps{i}\) represents the mean, and \(\Psi_{i}\) represents the noise covariance matrix for the \(i\)-th view.

\citet{bach2005probabilistic} established that the maximum likelihood solution of this model relates the \acrshort{cca} weights to the loadings by the within-view covariance \(\Sigma_{ii}\):

\begin{align}\label{eq:weights-to-loadings}
    W\sps{i} = \Sigma_{ii} U\sps{i} R
\end{align}

Where \(R\) is an arbitrary rotation matrix and \(U\sps{i}\) is the matrix of \acrshort{cca} weights for the \(i\)-th view.
For invertible covariance matrices, we can access an estimate of the `true' \acrshort{cca} weights associated with the top-k subspace by multiplying the loadings by the inverse of the covariance matrix:

\begin{align}\label{eq:loadings-to-weights}
    \hat{U}\sps{i}R = \Sigma_{ii}^{-1} W\sps{i}
\end{align}

We estimate the covariance matrices \(\Sigma_{ii}\) from the data using sample covariance matrices \(\hat{\Sigma}_{ii}\).
For Identity covariance matrices, the \acrshort{cca} weights are the same as the loadings.

In the Group Factor Analysis (GFA) model, we assume diagonal covariance, which aligns the marginal distribution of views with the probabilistic PCA model as the noise level decreases.
This assumption enhances computational efficiency and supports extensions like sparsity on loadings:

\begin{align}
    z &\sim \mathcal{N}(0, I)\\
    x\sps{i} &\sim \mathcal{N}(W\sps{i} z, {\sigma_{i}}^{2}I)
\end{align}

The joint distribution of the two views with an explicit latent variable model is:

\begin{align}
    \begin{bmatrix}
        X\sps{1} \\ X\sps{2}
    \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix}
                                              \mu\sps{1} \\ \mu\sps{2}
    \end{bmatrix}, \begin{bmatrix}
                       W\sps{1}W\spsT{1} + \Psi\sps{1} & W\sps{1}W\spsT{2} \\ W\sps{2}W\spsT{1} & W\sps{2}W\spsT{2} + \Psi\sps{2}
    \end{bmatrix} \right)
\end{align}

Where \(\Psi\sps{i}\) is the noise covariance matrix for the \(i\)-th view.
Importantly, this shows us that the true covariance in each view is a function of the loadings and the noise covariance matrix.
Specifically, the covariance matrix of the \(i\)-th view is given by:

\begin{align}
    \Sigma_{ii} = W\sps{i}W\spsT{i} + \Psi\sps{i}
\end{align}

When $W\sps{i}W\spsT{i}$ has large eigenvalues as compared to $\Psi\sps{i}$, $\Sigma_{ii}$ is often called a `spiked covariance matrix' \citep{johnstone2001distribution}.
Interestingly, as the noise level approaches zero, the marginal distribution of the views aligns with the probabilistic PCA model for each view.
In this case we don't even need to model the data as a multiview problem to recover the latent variables, and suggests we should generaly use PCA as a baseline.

\subsection{Implicit Latent Variable Models: The Joint Covariance Matrix Perspective}

The joint covariance matrix perspective, prevalent in sparse CCA literature \citep{suo2017sparse,chen2013sparse}, emphasizes covariance matrices over direct modeling of latent variables.
This approach is advantageous for constructing data with known sparse weights and canonical correlations.
This is achieved by constructing the joint covariance matrix of the distribution \(P(X\sps{1},X\sps{2})\):

\begin{align}
    \begin{bmatrix}
        X\sps{1} \\ X\sps{2}
    \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix}
                                              0 \\ 0
    \end{bmatrix}, \begin{bmatrix}
                       \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22}
    \end{bmatrix} \right)
\end{align}

Where \(\Sigma_{11}\) and \(\Sigma_{22}\) are the within-view covariance matrices and \(\Sigma_{12}\) and \(\Sigma_{21}\) are the between-view covariance matrices.

For clarity and simplicity in our discussion, we refer to a single canonical correlation coefficient, \(\rho\), without loss of generality.
This allows us to focus on the structure of the covariance matrices without the complexity of multiple canonical correlations.

In constructing the between-view covariance matrices \(\Sigma_{12}\) and \(\Sigma_{21}\), we control the true signal by setting active variables and correlations.
Specifically, the between-view covariance matrix is constructed as follows:

\begin{align}
    \Sigma_{12} = \rho \Sigma_{11} u\sps{1}_{1} u\spsT{2}_{1} \Sigma_{22}
\end{align}
%
Here, \(\rho\) is the canonical correlation, and \(u\sps{i}_{1}\) is the first column of the matrix of weights \(U\sps{i}\) for the \(i\)-th view.

This perspective simplifies the structure of covariance matrices, focusing on the relationship between views as controlled by the canonical correlation coefficient, \(\rho\), and the weights \(u\sps{i}\).

\subsection{Summary of Data Generation Methods}

In the following tables, we compare the covariance structures and the relationship between weights and loadings across these data generation methods.
These comparisons highlight key differences in modeling the relationships within and between views, and their implications for interpreting CCA in different scenarios.

Table \ref{tab:covariance-structures} summarizes the joint covariance structures of each data generation method.

\renewcommand{\arraystretch}{2.5} % Increase the row height
\begin{table}[h]
    \centering
    \caption{Covariance Structures in Data Generation Methods}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{}                                           & \textbf{Method}              & \textbf{Within-view Covariance} $\boldsymbol{\Sigma_{ii}}$ & \textbf{Between-view Covariance} $\boldsymbol{\Sigma_{12}}$ \\
        \hline
        \multirow{2}{*}{\rotatebox[origin=c]{90}{Explicit}} & Probabilistic \acrshort{cca} & $W\sps{i}W\spsT{i} + \Psi\sps{i}$ & $W\sps{1}W\spsT{2}$ \\
        \cline{2-4}
        & \acrshort{gfa}               & $W\sps{i}W\spsT{i} + {\sigma\sps{i}}^2 I$                    & $W\sps{1}W\spsT{2}$                                                 \\
        \hline
        \multirow{2}{*}{\rotatebox[origin=c]{90}{Implicit}} & Joint Covariance             & $\Sigma_{ii}$ & $\rho \Sigma_{11}u\sps{1}_1 u\spsT{2}_1 \Sigma_{22}$ \\
        \cline{2-4}
        & Joint Covariance (Identity)  & $I$                                                        & $\rho u\sps{1}_1 u\spsT{2}_1$                       \\
        \hline
    \end{tabular}
    \label{tab:covariance-structures}
\end{table}

Table \ref{tab:weights-loadings-population-sample} summarizes the relationship between the weights and loadings in each data generation method, distinguishing between population and sample cases.
This distinction is crucial, especially in scenarios where the population covariance matrix \( \Sigma \) is identity, but the sample covariance matrix \( \hat{\Sigma} \) is only an approximation.
An important observation is that for the implicit latent variable models, we can generate data with sparse weights but not, in general, sparse loadings.
For the explicit latent variable models, we can generate data with sparse loadings but not, in general, sparse weights.

\begin{table}[h]
    \centering
    \caption{Relationship Between Weights and Loadings in Population and Sample Cases}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{}                                           & \textbf{Method}                 & \textbf{Case} & \textbf{Weights}                            & \textbf{Loadings}                \\
        \hline
        \multirow{4}{*}{\rotatebox[origin=c]{90}{Explicit}} & Probabilistic \acrshort{cca} & Population & $(W\sps{i}W\spsT{i} + \Psi\sps{i})^{-1}W\sps{i}$ & $W\sps{i}$ \\
        &                                 & Sample        & $\hat{\Sigma_{ii}}^{-1}W\sps{i}$             & $W\sps{i}$                        \\
        \cline{2-5}
        & \acrshort{gfa}                  & Population    & $(W\sps{i}W\spsT{i} + {\sigma\sps{i}}^2 I)^{-1}W\sps{i}$      & $W\sps{i}$                        \\
        &                                 & Sample        & $\hat{\Sigma_{ii}}^{-1}W\sps{i}$             & $W\sps{i}$                        \\
        \hline
        \multirow{4}{*}{\rotatebox[origin=c]{90}{Implicit}} & Joint Covariance (Non-Identity) & Population & $U\sps{i}$ & $\Sigma_{ii}U\sps{i}$ \\
        &                                 & Sample        & $U\sps{i}$                                   & $\hat{\Sigma_{ii}}\hat{U\sps{i}}$ \\
        \cline{2-5}
        & Joint Covariance (Identity)     & Population    & $U\sps{i}$                                   & $U\sps{i}$                        \\
        &                                 & Sample        & $U\sps{i}$                                   & $\hat{\Sigma_{ii}}\hat{U\sps{i}}$ \\
        \hline
    \end{tabular}
    \label{tab:weights-loadings-population-sample}
\end{table}

\subsection{Regularization and Generative Models}

Regularization plays a crucial role in managing the complexity of machine learning models and preventing overfitting. In this subsection, we contrast the generative perspectives on Canonical Correlation Analysis (CCA) with the generative model for linear regression to highlight the differences in how regularization is interpreted in these models. This comparison provides insights into the implications of regularization for model interpretation and the identifiability of weights in CCA.
We show that in linear regression, regularization can be interpreted as prior on the weights, whereas in CCA, we can interpret regularization as a prior on the loadings or the weights, with different implications for model interpretation.
We also highlight the non-identifiability issue of the implicit latent variable model, where weights are not unique.

\subsubsection{Regularization and the Generative Model for Linear Regression}
Linear regression assumes data generation from a linear model with added noise:

\begin{align}
    y = xU + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I)
\end{align}

Here, $y$ are samples of the target variable, $x$ samples from the data matrix, $U$ the regression coefficients, and $\epsilon$ represents independent and identically distributed (i.i.d.) Gaussian noise.

\paragraph{Lasso Regression}
The Lasso imposes a Laplace prior on the regression coefficients, leading to a double-exponential prior on weights:

\begin{align}
    U \sim \mathcal{L}(0, \lambda)
\end{align}

\paragraph{Ridge Regression}
Ridge regression, in contrast, employs a Gaussian prior on the regression coefficients, equivalent to a Gaussian prior on weights:

\begin{align}
    U \sim \mathcal{N}(0, \lambda)
\end{align}

\subsubsection{Regularization and Generative Models for CCA}
CCA models differ in their approach to regularization compared to linear regression because they are latent variable models.

\paragraph{Explicit Latent Variable Model}
Regularization in the context of the explicit latent variable naturally relates to priors on the loadings \(W\sps{i}\).
For example, sparsity in the loadings can be achieved by imposing a Laplace prior on the loadings:

\begin{align}
    W\sps{i} \sim \mathcal{L}(0, \lambda)
\end{align}

This expresses the prior belief that latent factors only explain the data through a small number of features.
For example, in the context of latent factors in brain-behavior studies, this prior belief is equivalent to the assumption that a latent mode of variance (perhaps a subtype) is only expressed through a small number of brain regions.

\paragraph{Implicit Latent Variable Model}
In the implicit latent variable model of CCA, the joint likelihood of all observed \(x\) is modeled with \(\Sigma\) as a block covariance matrix, constructed based on the weights \(U\sps{i}\)
The covariance matrix \(\Sigma\) is structured to encapsulate both within-view variances and cross-covariances derived from the weights:

\begin{equation}
    \Sigma = \begin{bmatrix}
                 \Sigma_{1} & \Sigma_{1} U\sps{1} \rho U\spsT{2} \Sigma_{2} \\
                 \Sigma_{2} U\sps{2} \rho U\spsT{1} \Sigma_{1} & \Sigma_{2}
    \end{bmatrix}
\end{equation}

Here, \(\Sigma_{i}\) represents the within-view covariance matrix for view \(i\), and the off-diagonal blocks \(\Sigma_{1} U\sps{1} \rho U\spsT{2} \Sigma_{2}\) and its transpose represent the between-view covariance matrices.
These matrices are functions of the weights \(U\sps{i}\) and their interrelationships across different views of the data, modulated by \(\rho\), the canonical correlation coefficients.

Here the regularization naturally relates to priors on the weights \(U\sps{i}\).
For example, sparsity in the weights can be achieved by imposing a Laplace prior on the weights:

\begin{align}
    U\sps{i} \sim \mathcal{L}(0, \lambda)
\end{align}

This expresses the more nuanced prior belief that the latent factors are expressed through a subset of features and then distorted by arbitrary rotations as well as the within-view covariance matrices.
Manipulating equation \ref{eq:weights-to-loadings}, the conditional distribution of the implicit latent variable model we have:

\begin{align}
    x\sps{i} | z &\sim \mathcal{N}(\Sigma_{i}U\sps{i}R z=W\sps{i}z, \Sigma_{i}-W\sps{i}W\spsT{i}=\Psi\sps{i}) \\
    z &\sim \mathcal{N}(0, I)
\end{align}

The arbitrary rotation matrix \(R\) means that for multidimensional $U\sps{i}$, even if $\Sigma_{i}=I$, and even if the true loadings are sparse, the weights may still not be sparse!

\begin{align}
    x\sps{i} | z &\sim \mathcal{N}(U\sps{i}R z=W_{sparse}\sps{i}z, \Sigma_{i}-W_{sparse}\sps{i}W_{sparse}\spsT{i}=\Psi\sps{i}) \\
    z &\sim \mathcal{N}(0, I)
\end{align}

Alternatively, even if we know the true weights (i.e. $R=I$), the CCA model may not be able to recover them.
This is to say they are not, in general, identifiable \citep{park2023critical}.
In other words there are multiple values of $W\sps{i}$ that can produce the same covariance structure.

We can illustrate this with a trivial example:

\begin{align}
    \Sigma_{1} &= \begin{bmatrix}
                         1 & 1 & 0 \\
                         1 & 1 & 0 \\
                         0 & 0 & 1
    \end{bmatrix}\begin{bmatrix}
                     1 \\
                     0 \\
                     1
    \end{bmatrix}=
    \begin{bmatrix}
        1 & 1 & 0 \\
        1 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}\begin{bmatrix}
                     0.5 \\
                     0.5 \\
                     1
    \end{bmatrix}= \begin{bmatrix}
                        1 \\
                        1 \\
                        1
    \end{bmatrix} \\
\end{align}

In this example, we show that the same covariance matrix $\Sigma_1$ can be obtained using different weight matrices. The first weight matrix has entries [1, 0, 1], while the second weight matrix has entries [0.5, 0.5, 1]. This demonstrates that there are multiple weight matrices that can produce the same covariance structure, highlighting the non-identifiability issue in the implicit latent variable model.

One practical implication of this observation is that it raises serious questions about using stability selection, a common practice in the sparse \acrshort{cca} literature \citep{mihalik2020multiple, deng2021sparse}, to select the optimal regularization parameter.
This is because different runs of the algorithm may result in different rotations of the weights, even if the representations (and therefore correlations) are the same!
At best this could lead to suboptimal hyperparameter selection and at worst it could lead to incorrect conclusions about the structure of the data.

Whereas priors on the loadings in the explicit latent variable model are intuitive, priors on the weights in the implicit latent variable model are evidently less so.

\section{Proving the Invariance of Loadings in CCA}

Building on these observations, we now shift our focus to a mathematical argument favoring the use of \gls{loadings} over weights for the interpretation of \acrshort{cca} models.
Specifically, we demonstrate that \gls{loadings} are invariant to columnwise transformations of the data matrix, a property not shared by weights.
\subsection{CCA in the Principal Component Space}
We begin by showing that CCA can be solved in the principal component space. Consider the singular value decomposition (SVD) of the data matrices:

\begin{align}
    X\sps{i} = U\sps{i}\Sigma_{i}V^{i\top} \label{eq:svd}
\end{align}

Here, $U\sps{i}$ and $V\sps{i}$ are the left and right singular vectors of $X\sps{i}$ respectively, and $\Sigma_{i}$ is a diagonal matrix of singular values. This decomposition represents the data matrix in terms of its fundamental components: the directions of maximum variance (captured by $V\sps{i}$), the scale of these directions (captured by $\Sigma_{i}$), and the projections of the data onto these directions (captured by $U\sps{i}$). The left singular vectors $U\sps{i}$ are the principal components of $X\sps{i}$.

Substituting Equation \ref{eq:svd} into the \acrshort{cca} objective function, we have:

\begin{align}
\max_{U\sps{1}, u\sps{2}} \Corr(X\sps{1}U\sps{1}, X\sps{2}u\sps{2}) &= \max_{U\sps{1}, u\sps{2}} \Corr(U\sps{1}\Sigma_{1}V^{1\top}U\sps{1}, U\sps{2}\Sigma_{2}V^{2\top}u\sps{2}) \label{eq:cca_obj}
\end{align}

By reparameterizing the weights as $v\sps{i} = \Sigma_{i}V^{i\top}u\sps{i}$, we obtain:

\begin{align}
\max_{v\sps{1}, v\sps{2}} \Corr(U\sps{1}v\sps{1}, U\sps{2}v\sps{2}) \label{eq:reparam}
\end{align}

This reparameterization simplifies the optimization problem in two ways. First, if the data matrices are low-rank (which is guaranteed if the number of samples is less than the number of features), then the matrix of principal components $U\sps{i}$ is lower dimensional than the data matrix $X\sps{i}$, reducing the number of parameters in the optimization problem. Second, the reparameterization ensures that $v\spsT{1}U\spsT{1}U\sps{1}v\sps{1}=v\spsT{1}v\sps{1}$, making the constraints independent of the data. Therefore, we can solve the CCA problem by solving the simpler PLS problem in the principal component space, which is computationally more feasible and provides a convenient way to understand how the weights and \gls{loadings} of \acrshort{cca} models change under different transformations of the data.

\subsection{Definition of Loadings}
We define \textit{loadings} using the reparameterized weights as follows:

\begin{align}
w\sps{i}_j = \Corr(X\sps{i}_j, U\sps{i}v\sps{i}) = \frac{\Cov(X\sps{i}_j, U\sps{i}v\sps{i})}{\sqrt{\Var(X\sps{i}_j)}\sqrt{\Var(U\sps{i}v\sps{i})}} \label{eq:loading_def}
\end{align}

By convention, and without loss of generality, we standardize the latent variables to have unit variance, so that:

\begin{align}
w\sps{i}_j = \frac{\Cov(X\sps{i}_j, U\sps{i}v\sps{i})}{\sqrt{\Var(X\sps{i}_j)}} \label{eq:standardized_loading}
\end{align}

Intuitively, \gls{loadings} measure how much each original feature contributes to the latent variables, providing insight into the structure of the data.

\subsection{Scale Invariance of Loadings in CCA}

We now show that the \gls{loadings} are invariant to column-wise scaling of the data matrix, whereas the weights are not.

\subsubsection{Effect of Scaling on Singular Vectors}

\begin{lemma}
Scaling the columns of the data matrix does not affect the left singular vectors $U\sps{i}$.
\end{lemma}

\begin{proof}
Let $C$ be a diagonal matrix that scales the columns of the data:

\begin{equation}
    C = \begin{pmatrix}
            c_{11} & 0      & \cdots & 0      \\
            0      & c_{22} & \cdots & 0      \\
            \vdots & \vdots & \ddots & \vdots \\
            0      & 0      & \cdots & c_{nn}
    \end{pmatrix}
\end{equation}

where \( c_{ii} \) represents the scaling factor for the \( i \)-th column of the data matrix. For columns that are not scaled, \( c_{ii} = 1 \), meaning that the corresponding column remains unchanged.

Since $C$ is diagonal, it can be represented by a diagonal matrix $S=C$ and an orthogonal matrix (the identity matrix) $R$. The transformed dataset is therefore \( X^{(1')} = X^{(1)}C \):

\begin{align}
    X\sps{1'} = U\sps{i}\Sigma_{i}V^{i\top}C = U\sps{i}(\Sigma_{i} S\sps{i}) (\sps{i}V^{i\top}I) = U\sps{i} \Sigma_{i'} (V\sps{i})^T \label{eq:scaled_data}
\end{align}

where $\Sigma_{i'}=\Sigma_{i} S\sps{i}$. This makes it clear that the left singular vectors \( U\sps{i} \) remain unchanged.
\end{proof}.

Noting that the CCA optimization problem remains the same as in Equation \ref{eq:reparam}, we can now show that the weights are not invariant to scaling of the data matrix, but the \gls{loadings} are.

\subsubsection{Weights Change Under Scaling}

From the earlier reparameterization, and given that $v\sps{i'}=v\sps{i}$, the weights post-scaling are:

\begin{align}
u\sps{i'} = V\sps{i}(\Sigma_{i'})^{-1}v\sps{i} = V\sps{i}(\Sigma_{i'})^{-1}v\sps{i} = C^{{(i)}^{-1}}u\sps{i} \label{eq:weights_change2}
\end{align}

which are the original weights scaled by the inverse of the scaling matrix ( C ). This means that the weights are not invariant to scaling of the data matrix. Furthermore, it means we can set the weights to arbitrary values by scaling the data matrix.

\subsubsection{Invariance of Loadings Under Scaling}

Since \gls{loadings} are correlations between the original features and the latent variables, they are invariant to scaling of the data. This follows from the definition of correlation and the fact that the latent variables remain unchanged:

\begin{align}
w\sps{i}_j = \Corr(X\sps{i'}j, U\sps{i}v\sps{i}) = \Corr(c{jj}X\sps{i}_j, U\sps{i}v\sps{i}) = \Corr(X\sps{i}_j, U\sps{i}v\sps{i}) \label{eq:loading_invariant}
\end{align}

Intuitively, the \gls{loadings} remain the same because scaling the data does not change the relative contributions of each feature to the latent variables.

\subsection{Invariance to Repeated Linear Combinations of Columns}\label{subsubsec:invariance-to-linear-combinations}

We can also prove a more general result that the \gls{loadings} are invariant to repeated linear combinations of columns of the data matrix. This is particularly relevant when we need to decide which features to include or exclude in a model, or when working with highly correlated variables like survey questions, where we may choose to use summary scores instead of individual items.

\subsubsection{Impact of Linear Combinations on Singular Vectors}

\begin{lemma}
Adding linear combinations of columns to the data matrix does not affect the left singular vectors $U\sps{i}$.
\end{lemma}

\begin{proof}
    Consider adding columns that are linear combinations of existing columns in \( X\sps{i} \) to form \( X\sps{i''} \).
    We can represent this using a transformation matrix \( A \) such that \( X\sps{i''} = X\sps{i}A \):

    \begin{equation}
        A = \begin{pmatrix}
                1      & 0      & \cdots & 0      & a_{11} & a_{12} & \cdots & a_{1m} \\
                0      & 1      & \cdots & 0      & a_{21} & a_{22} & \cdots & a_{2m} \\
                \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                0      & 0      & \cdots & 1      & a_{n1} & a_{n2} & \cdots & a_{nm}
        \end{pmatrix}
    \end{equation}
    
    where \( a_{ij} \) represents the weight of the \( j \)-th column in the \( i \)-th linear combination. The key is that we can still represent the transformed dataset as a product of the original left singular vectors \( U\sps{i} \) and new matrices \( \Sigma_{i''} \) and \( V\sps{i''} \):
    
    \begin{align}
        X\sps{i''} = U\sps{i}(\Sigma_{i}V^{i\top}A) = U\sps{i}(\Sigma_{i'}V\spsT{i'})  \label{eq:svd_linear_combinations}
    \end{align}
    
    This transformation is rank-preserving because the first \( n \) columns of \( A \) form the identity matrix. Therefore, the left singular vectors \( U\sps{i''} \) remain the same as \( U\sps{i} \).
\end{proof}

\subsubsection{Weights are Underdetermined}

The weights $u\sps{i''}$ are underdetermined in the transformed space due to the added linear dependencies in the columns. The specific weights will depend on the SVD computation approach.

\begin{align}
u\sps{i''} = V\sps{i}(\Sigma_{i''})^{-1}v\sps{i} \label{eq:weights_change}
\end{align}

In the extreme case, if we have two identical columns in the data matrix, we can use any weights for these columns provided that their sum is the same.

\subsubsection{Invariance of Loadings}

The loadings, as before, remain unchanged because the original columns are unchanged and the latent variables are unchanged.

\subsection{Summary}

In this section, we have established the mathematical basis for preferring \gls{loadings} over weights in interpreting \acrshort{cca} models. We demonstrated that \gls{loadings} are invariant to columnwise transformations of the data matrix, a property not shared by weights. This distinction is not just of theoretical interest but has significant practical implications in the application of CCA.

\subsubsection{Practical Implications}

While the identifiability of weights can be partially solved by the standardization of data, and while this is a common practice, it is not always necessary or desirable and always introduces assumptions. Additionally, in fields like psychometrics or social sciences, where survey data is often used, the decision to include or exclude specific features, or to use composite scores instead of individual items, can significantly affect the analysis. The invariance of \gls{loadings} to such alterations in the data structure makes them a more robust choice for interpreting relationships between variables in these contexts.

In summary, the preference for \gls{loadings} in the interpretation of \acrshort{cca} models is not only mathematically sound but also practically advantageous. It provides a more reliable and consistent framework for interpreting the relationships between variables in complex datasets, especially in interdisciplinary research, data preprocessing, and fields dealing with heterogeneous or transformed data.

\section{Efficient Sampling of Simulated CCA Data}\label{sec:efficient}

Efficient sampling from high-dimensional multivariate normal distributions is a critical step in simulating data for Canonical Correlation Analysis (CCA). Traditional methods can be computationally intensive and storage-demanding, especially for large datasets.
This has in practice limited the dimensionality of simulated data, restricting the scope of research and analysis.
For example \citet{matkovic2023contribution} simulate data with 8,000 observations and 100 features while \citet{helmer2020stability} used at most 10,000 observations and 64 features.
We were interested in the behavior of CCA in high-dimensional settings like voxel-wise MRI and connectivities, which can have hundreds of thousands of features \citep{jack2008alzheimer} and up to tens of thousands of observations \citep{sudlow2015uk}.
To address this challenge, we make the assumption that in biomedical data, the covariance matrix is low-rank and/or sparse, and use this to develop efficient methods for sampling from multivariate normal distributions.

\subsection{Challenges with High-Dimensional Data}
Direct sampling from a multivariate normal distribution is impractically slow for high-dimensional data\footnote{This has been arguably \textit{the} core research challenge for Monte Carlo methods \citep{mackay1998introduction}}.
In particular, the implicit latent variable model requires storage of the full covariance matrix, which is prohibitive for high-dimensional data.
This is because storing a covariance matrix with, for example, 100,000 dimensions would require 80GB of memory to store.
This is impractical for many computers let alone laptops.

\subsection{Sampling from Multivariate Normal Distributions}
An efficient approach to sampling from a multivariate normal distribution is to use the Singular Value Decomposition (SVD) or cholesky decomposition of the covariance matrix.
This approach involves decomposing the covariance matrix and then using the resulting components to transform samples from a standard multivariate normal distribution, thereby generating samples that conform to the desired high-dimensional distribution with reduced computational overhead.

\begin{align}
    Z~\sim \mathcal{N}(0, I) \\
    X = \Sigma^{1/2}Z
\end{align}

Where \( \Sigma^{1/2} \) is a square root of the covariance matrix.
Notice that this is exactly the same as the generative model for the explicit latent variable model, where \( \Sigma^{1/2} \) is the matrix of loadings.
Note that we can also add low rank noise by sampling from an independent multivariate normal distribution and adding it to the transformed samples.
This means we only need to sample from a univariate normal distribution and perform a matrix multiplication of complexity \(O(np^2)\).
However, even with this approach, the computational complexity and storage requirements can still be prohibitive for high-dimensional data.
In particular, the implicit latent variable model requires storage of the full covariance matrix.
For example, a covariance matrix with 100,000 dimensions would require 80GB of memory to store.
This is impractical for many computers let alone laptops.

\subsection{Using Low-Rank Covariance Matrices}
The explicit latent variable model, offers us more efficient approaches
We employ two strategies: sparse and low-rank covariance matrices.
For certain applications, sparse covariance matrices offer an additional avenue for efficiency.
These matrices, with many zero entries, reduce both computational complexity and storage requirements, allowing for faster processing and less memory usage.
For example, a sparse covariance matrix with 100,000 dimensions and 10\% density would only require 8GB of memory to store.
Using low-rank covariance matrices, we can reduce the complexity further by storing only the factorized rank-$k$ components.
In this way we can reduce the storage requirements to at most \(O(kp)\).
For example, a low-rank covariance matrix with 100,000 dimensions, 10\% density and rank 1000 would only require 80MB of memory to store.
We also only need to draw \(O(kp)\) samples from a univariate normal distribution and perform a matrix multiplication with complexity \(O(nkp)\) rather than \(O(np^2)\) for the full rank case.

\subsection{Calculating the True Canonical Correlations}
We can also control the population canonical correlations by varying the signal-to-noise ratio (SNR) i.e. the ratio of the variance of the signal to the variance of the noise (the sum of the eigenvalues of the covariance matrices).

\subsection{Calculating the True Weights (and Loadings)}
We get the loadings for free (they are the low-rank square root of the covariance matrix).
For the weights, we can use the relationship between the weights and the loadings in the explicit latent variable model to calculate the weights from the loadings and the covariance matrix.
Recall that the weights are given by:

\begin{align}
    \hat{W}\sps{i} = \Sigma_{ii} \hat{U}\sps{i} R
\end{align}

Where $R$ is an arbitrary rotation matrix and $\hat{U}\sps{i}$ is the matrix of \acrshort{cca} weights for the $i$th view.
This implies that for invertible covariance matrices, we can access the `true' \acrshort{cca} weights associated with the top-k subspace by multiplying the \gls{loadings} by the inverse of the covariance matrix:

\begin{align}
    \hat{U}\sps{i}R = \Sigma_{ii}^{-1} \hat{W}\sps{i}
\end{align}

The apparent problem is that we need to invert the \(O(p^2)\) covariance matrix, which is computationally expensive.
However, we can use the Sherman-Morrison-Woodbury formula to calculate the inverse of the covariance matrix in \(O(kp^2)\) time, where \(k\) is the rank of the covariance matrix.
This is because the inverse of a rank-k matrix can be written as a rank-1 update of the inverse of a rank-(k-1) matrix.
This means that we can calculate the weights in \(O(kp^2)\) time, which is much faster than the \(O(p^3)\) time required to calculate the weights directly from the covariance matrix.

In the sections so far, we have discussed the mathematical properties of weights and \gls{loadings} in \acrshort{cca} models and categorized the different generative models for \acrshort{cca}.
We have also shown how to efficiently sample from these generative models.
In the next section, we will present a number of experiments demonstrating the relationship between weights and \gls{loadings} in simulated data.

\section{Experiment Design}

Our goal in this section is to empirically demonstrate the relationship between weights and \gls{loadings} in \acrshort{cca} models as well as to better understand the behavior of \acrshort{cca} models in the high-dimensional settings that section \ref{sec:efficient} enables, and which are of interest in the neuroimaging community.

The first set of experiments illustrates the relationship between weights and \gls{loadings} in simulated data using explicit latent variable models with identity and non-identity covariance matrices.
The second set of experiments illustrates the ammount of information that can be recovered from simulated data using \acrshort{cca} and \acrshort{pls} models with varying signal-to-noise ratios and sample sizes.

\subsection{Exploring the Relationship Between Weights and Loadings in CCA Using Simulated Data}

Our first experiment is designed to illustrate the challenges of recovering the true weights and \gls{loadings} respectively in \acrshort{cca} models for explicit and implicit latent variable models with identity and non-identity covariance matrices.

We compare the true weights derived from the data generation model with the estimated weights of CCA, Ridge CCA, Elastic Net, PLS, and PCA models.
We expect that when the covariance matrix is identity, the weights and \gls{loadings} will be identical.
When the covariance matrix is non-identity, we expect that the weights and \gls{loadings} will be different.
Moreover, we expect that the estimated loadings will be more stable than the estimated weights for CCA models because the weights are not always identifiable.
Under the explicit latent variable model, we expect that the weights will only be (close to) sparse when the covariance matrix is close to identity.
This means we do not expect the Elastic Net model to improve on the Ridge CCA model since the Lasso regularizes the weights but not the \gls{loadings}.
Finally, we expect that when using the explicit latent variable model, for high signal-to-noise ratios, the PLS and even PCA models will recover the true weights and \gls{loadings} because the majority of the variance is explained by the latent variables.

\paragraph{Detailed Parameters of Simulated Data for Weights and Loadings Analysis in CCA} We generate data with 100 samples and 10 features in each view.
We then generate data under two implicit latent variable models and two explicit latent variable models.
The ridge penalty is coarsely tuned between 0.1 and 0.9 in order to illustrate the effect of regularization as we already show the corner cases of no regularization (CCA) and full regularization (PLS).
For the Elastic Net model, we tune the l1 ratio between 0.1 and 0.9. This ensures that the Elastic Net has some sparsity as compared to the Ridge model, effectively avoiding the corner case of no sparsity where the Elastic Net is equivalent to the Ridge model.
We summarize the parameters of these experiments in table \ref{tab:simulated-data-parameters}.

\begin{table}
    \centering
    \caption{Simulated Data Parameters for Weight and Loadings Recovery Experiments}
    \begin{tabular}{| l | l |}
        \hline
        \textbf{Parameter}                        & \textbf{Value}                               \\
        \hline
        Number of samples (\textit{n})            & 100 train, 500 test                            \\
        Number of features in View 1 (\textit{p}) & 10 \\
        Number of features in View 2 (\textit{q}) & 10 \\
        True Latent dimensions                    & 1                                            \\
        Fraction of active features (explicit:loadings, implicit:weights) View 1            & 0.5                                          \\
        Fraction of active features (explicit:loadings, implicit:weights) View 2            & 0.5                                          \\
        \hline
    \end{tabular}\label{tab:simulated-data-parameters}
\end{table}

\subsection{Assessing Information Recovery in CCA and PLS Models Under Varying Signal-to-Noise Ratios}

Our next experiment was motivated by the observation that \acrshort{pls} models (including sparse PLS) often exhibit low but non-zero out of sample correlations in real high-dimensional data.
We want to understand how much of this is due to the fact that PLS models optimize covariance rather than correlation, and how much is due to the fact that the signal-to-noise ratio is too low.
In order to understand this, we simulated data with varying signal-to-noise ratios and compared the out of sample correlations of \acrshort{pls} models with the out of sample correlations of Ridge \acrshort{cca} models with varying regularization.
Since we are interested in studying these effects in high-dimensional data, we aimed to simulate data with similar numbers of features to real brain-behavior datasets.
This means that we are only able to use our memory-efficient sampling methods for the explicit latent variable model.

\paragraph{Detailed Parameters of Simulated Data for Signal-to-Noise Simulations}
We simulated data with 1000 samples and between 100 and 10,000 features in one view and 100 features in the other.
These are of the same order of magnitude as typical brain-behaviour datasets.
We summarise these data properties in table \ref{tab:simulated-data-parameters-bb}.

\begin{table}
    \centering
    \caption{Simulated Data Parameters for Brain-Behaviour Simulations}
    \begin{tabular}{| l | l |}
        \hline
        \textbf{Parameter}                        & \textbf{Value}                               \\
        \hline
        Number of features in View 1 (\textit{p}) & 100-10000 \\
        Number of features in View 2 (\textit{q}) & 100-10000 \\
        True Latent dimensions                    & 1                                            \\
        Fraction of active features View 1            & 1.0                                          \\
        Fraction of active features View 2            & 1.0                                          \\
        Signal-to-noise ratio                    & 0.001-1 \\
        \hline
    \end{tabular}\label{tab:simulated-data-parameters-bb}
\end{table}

\subsection{Methodology for Constructing Correlated Covariance Matrices in CCA Simulations}

In both experiments, we construct correlated covariance matrices by generating a random matrix $A$ with entries drawn from a uniform distribution between -1 and 1.
We then construct the covariance matrix as $\Sigma = AA^\top$.
This ensures that the covariance matrix is positive semi-definite and also tends to produce strong correlations.

We plot an example of the covariance matrices for correlated covariance matrices in both views in figure \ref{fig:covariance-matrices}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/simulated/explicit/True_Covariance_Correlated.pdf}
    \caption{Example instances of correlated covariance matrices.}\label{fig:covariance-matrices}
\end{figure}

Recalling table \ref{tab:covariance-structures}, note that in the implicit latent variable models, these covariance matrices are precisely the population within-view covariance matrices.
In the explicit latent variable models, these covariance matrices are just the covariance matrices of the noise to which we add the signal covariance matrices.
Nonetheless, for strong enough noise, this process ensures that there are large correlations between features.

\section{Experiment Results}

\subsection{Exploring the Relationship Between Weights and Loadings in CCA Using Simulated Data}

We first present the results of the experiments demonstrating the relationship between weights and \gls{loadings} in simulated data from explicit and implicit latent variable models with identity and non-identity covariance matrices.

For both cases, we plot the true weights and loadings along with the estimated weights and loadings for each model.
We estimate model loadings by multiplying the model weights by the sample within-view covariance matrix following equation \ref{eq:weights-to-loadings}.
This means that the estimated model loadings may not be sparse even when the estimated model weights are sparse and the \textit{population} covariance matrix is identity.

\subsubsection{Implicit Latent Variables (Sparse Weights)}

Figure \ref{fig:implicit-weights-loadings} shows the true and estimated weights and \gls{loadings} for data generated from the implicit latent variable models with sparse weights.
The Elastic net model exhibits no false negatives (i.e. where the true weight is non-zero but the estimated weight is zero) in both cases.
This shows that the Elastic Net model is able to recover the true weights and that the Lasso penalty is indeed inducing sparsity in the weights.
The CCA model appears recover spectrum of the true weights much better for the identity covariance matrices than for the correlated covariance matrices.
This is likely because the multicollinearity introduced makes the learnt weights substantially less stable with respect to a change in the data.

\begin{figure}
\centering
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/simulated/implicit/Combined_Weights_Loadings_with_Error_Bars_Identity}
\end{subfigure}
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/simulated/implicit/Combined_Weights_Loadings_with_Error_Bars_Correlated}
\end{subfigure}
\caption{Bar plots of the true and estimated weights and \gls{loadings} for data generated from the implicit latent variable models with sparse weights. The left column shows the results for the identity covariance matrices, while the right column shows the results for the correlated covariance matrices.}\label{fig:implicit-weights-loadings}
\end{figure}

We can also quantify the similarity between the true and estimated weights and \gls{loadings} using the cosine similarity; a measure of the similarity between two vectors that is invariant to the scale of the vectors.
The cosine similarity between two vectors is defined as the cosine of the angle between them \citep{luo2018cosine}.
Since we are indifferent to the direction of the vectors, we take the absolute value of the cosine similarity.
The absolute cosine similarity between two vectors is 1 if they are identical (up top a sign) and 0 if they are orthogonal.
We plot the cosine similarity between the true and estimated weights and \gls{loadings} for data generated from the implicit latent variable models with sparse weights in figure \ref{fig:implicit-weights-loadings-cosine}.

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{figures/simulated/implicit/weight_loading_difference}
\caption{Cosine similarity between the true and estimated weights and \gls{loadings} for data generated from the implicit latent variable models with sparse weights. We plot each run as a point on a scatter plot with a log scale. The grey line indicates where the similarity between weights and loadings are equal.}\label{fig:implicit-weights-loadings-cosine}
\end{figure}

Interestingly, we see that for the identity covariance matrices, weight differences are smaller than loading differences.
On the other hand for the correlated covariance matrices, the loading differences are smaller than the weight differences.
This is evidence of the fact that the weights are not identifiable in the implicit latent variable model as suggested by our theory.
Only when the covariance matrices are identity, and when there is only one latent variable, are the weights identifiable.

\subsubsection{Explicit Latent Variables}

Figure \ref{fig:explicit-weights-loadings} shows the true and estimated weights and \gls{loadings} for data generated from the explicit latent variable models with sparse \gls{loadings}.
The left column shows the results for the identity covariance matrices, while the right column shows the results for the correlated covariance matrices.
Once again, the Elastic Net model exhibits no false negatives (i.e. where the true weight is non-zero but the estimated weight is zero) when the noise covariance matrix is identity such that both the weights and \gls{loadings} are sparse.


\begin{figure}
\centering
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/simulated/explicit/Combined_Weights_Loadings_with_Error_Bars_Identity}
\end{subfigure}
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/simulated/explicit/Combined_Weights_Loadings_with_Error_Bars_Correlated}
\end{subfigure}
\caption{Bar plots of the true and estimated weights and \gls{loadings} for data generated from the implicit latent variable models with sparse weights. The left column shows the results for the identity covariance matrices, while the right column shows the results for the correlated covariance matrices.}\label{fig:explicit-weights-loadings}
\end{figure}

Once again, we can quantify the similarity between the true and estimated weights and \gls{loadings} using the cosine similarity (Figure \ref{fig:explicit-weights-loadings-cosine}).

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{figures/simulated/explicit/weight_loading_difference}
\caption{Cosine similarity between the true and estimated weights and \gls{loadings} for data generated from the explicit latent variable models with sparse \gls{loadings}. We plot each run as a point on a scatter plot with a log scale. The grey line indicates where the similarity between weights and loadings are equal.}\label{fig:explicit-weights-loadings-cosine}
\end{figure}

Notably, when the noise covariance matrix is correlated, the difference in recovery of the weights is much larger than the difference in recovery of the \gls{loadings}.
Suprisingly, when the noise covariance matrix is identity, the PLS and PCA models appear to better recover the weights than the loadings in this case.

\subsection{Assessing Information Recovery in CCA and PLS Models Under Varying Signal-to-Noise Ratios}

In Figures \ref{fig:snr-scores-identity} and \ref{fig:snr-scores-random} we plot the test correlation (score) varying the signal-to-noise ratio and the number of features under the identity and correlated noise covariance matrices respectively.

In figure \ref{fig:snr-scores-identity}, we can see that the PLS model outperforms all of the Ridge CCA models for all values of the signal-to-noise ratio and dimensionality, though only by a small margin.
The unregeularized CCA model is much worse than even the Ridge CCA model with the smallest regularization.
In this experiment the performance of PLS is directly related to the signal-to-noise ratio.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/brain_behaviour_sim/snr_vs_scores_facet_identity}
    \caption{Varying signal to noise ratio with identity covariance matrices. We plot the performance of different levels of Regularized CCA from 0 (CCA) to 1 (PLS) for different sample sizes. }\label{fig:snr-scores-identity}
\end{figure}

In figure \ref{fig:snr-scores-random}, we see a totally different picture.
The PLS model is now outperformed by the Ridge CCA model with the smallest regularization.
While CCA is still the worst performing model, PLS is now much worse across signal-to-noise ratios and dimensions than any of the Ridge CCA models.
This suggests that the PLS model is not able to recover anything like the true signal when the covariance matrices are correlated.


In this experiment it is also clear that the signal-to-noise ratio must be higher to obtain the same performance with higher dimensional data.
It is interesting that performance of the Ridge CCA improves across the board with lower regularization.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/brain_behaviour_sim/snr_vs_scores_facet_random}
    \caption{Varying signal to noise ratio with correlated covariance matrices. We plot the performance of different levels of Regularized CCA from 0 (CCA) to 1 (PLS) for different sample sizes.}\label{fig:snr-scores-random}
\end{figure}

\section{Discussion}

\subsection{Limitations}


\subsection{Future Work}

Given our theoretical observations in this chapter, a natural question to ask is whether we can construct a regularization functional that imposes sparsity on the \gls{loadings} (instead of the weights).
The answer is yes, but it is not straightforward and in the small sample setting, it is not clear that it is a good idea.
The principle would be much the same as the Lasso, but we would need to use the sample covariance matrix to define the norm:

\begin{align}
    P(W)=\|W\|_1 \\
    P(L)=\|\hat{\Sigma}U\|_1
\end{align}

Which imposes an L1 penalty on the \gls{loadings} via an L1 penalty on the \gls{weights} multiplied by the sample covariance matrix.
We could in principle apply the soft-thresholding operator to the estimated loadings.
However we would need to be careful to ensure that the sample covariance matrix is invertible in order to get back to the weights.
This is of course not guaranteed in the small sample setting.

\subsection{Conclusion}

In this chapter, we explored the relationship between weights and loadings in CCA models from both theoretical and empirical perspectives. We unified methods for generating simulated multiview data using implicit and explicit latent variable models, providing a framework for understanding the properties of CCA and PLS models.

Through a rigorous mathematical argument, we demonstrated that loadings are invariant to columnwise transformations of the data matrix, while weights are not. This invariance property makes loadings a more reliable choice for interpreting CCA models, as the weights can be arbitrarily set by scaling the data matrix or adding linear combinations of columns.

Our experiments using simulated data provided empirical evidence supporting the theoretical findings. We showed that the recovery of true weights and loadings depends on the underlying covariance structure and the choice of regularization. The results highlighted the importance of considering the signal-to-noise ratio and dimensionality when applying CCA and PLS models to real-world datasets.

Furthermore, we discussed the possibility of constructing a regularization functional that imposes sparsity on loadings instead of weights. While this approach is theoretically feasible, it presents challenges in the small sample setting and requires further investigation.

Overall, this chapter contributes to a better understanding of the behavior and interpretation of CCA models, providing valuable insights for researchers and practitioners working with multiview data. The findings emphasize the importance of considering the invariance properties of loadings and the impact of covariance structure and regularization on model performance. Future research could explore the extension of these insights to more complex data scenarios and the development of efficient algorithms for imposing sparsity on loadings in CCA models.